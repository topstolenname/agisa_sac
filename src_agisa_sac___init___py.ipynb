{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/topstolenname/agisa_sac/blob/codex%2Fresolve-identity-conflicts-and-errors/src_agisa_sac___init___py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/__init__.py\n",
        "\"\"\"\n",
        "AGI-SAC Simulation Framework\n",
        "----------------------------\n",
        "\n",
        "A multi-agent simulation framework for exploring emergent cognition,\n",
        "distributed identity, and Stand Alone Complex phenomena.\n",
        "\"\"\"\n",
        "\n",
        "__version__ = \"1.0.0-alpha\"\n",
        "FRAMEWORK_VERSION = f\"AGI-SAC v{__version__}\"\n",
        "\n",
        "# Expose key classes for easier import from the top-level package\n",
        "try:\n",
        "    from .orchestrator import SimulationOrchestrator\n",
        "    from .agent import EnhancedAgent\n",
        "    from .components.memory import MemoryContinuumLayer, MemoryEncapsulation\n",
        "    from .components.cognitive import CognitiveDiversityEngine\n",
        "    from .components.social import DynamicSocialGraph\n",
        "    from .components.resonance import TemporalResonanceTracker, ResonanceLiturgy\n",
        "    from .components.voice import VoiceEngine\n",
        "    from .components.reflexivity import ReflexivityLayer\n",
        "    from .analysis.analyzer import AgentStateAnalyzer\n",
        "    from .analysis.exporter import ChronicleExporter\n",
        "    from .analysis.tda import PersistentHomologyTracker\n",
        "    from .analysis.visualization import plot_persistence_diagram, plot_persistence_barcode, plot_metric_comparison\n",
        "    from .analysis.clustering import cluster_archetypes\n",
        "    from .utils.message_bus import MessageBus\n",
        "except ImportError as e:\n",
        "    import warnings\n",
        "    warnings.warn(f\"Could not import all AGI-SAC components during package initialization: {e}\", ImportWarning)\n",
        "\n",
        "# Define __all__ for explicit public API if desired\n",
        "__all__ = [\n",
        "    \"FRAMEWORK_VERSION\",\n",
        "    \"SimulationOrchestrator\",\n",
        "    \"EnhancedAgent\",\n",
        "    \"MemoryContinuumLayer\",\n",
        "    \"MemoryEncapsulation\",\n",
        "    \"CognitiveDiversityEngine\",\n",
        "    \"DynamicSocialGraph\",\n",
        "    \"TemporalResonanceTracker\",\n",
        "    \"ResonanceLiturgy\",\n",
        "    \"VoiceEngine\",\n",
        "    \"ReflexivityLayer\",\n",
        "    \"AgentStateAnalyzer\",\n",
        "    \"ChronicleExporter\",\n",
        "    \"PersistentHomologyTracker\",\n",
        "    \"MessageBus\",\n",
        "    \"plot_persistence_diagram\",\n",
        "    \"plot_persistence_barcode\",\n",
        "    \"plot_metric_comparison\",\n",
        "    \"cluster_archetypes\",\n",
        "]\n",
        "\n",
        "# Optional: Basic logging setup for the library\n",
        "import logging\n",
        "logging.getLogger(__name__).addHandler(logging.NullHandler())\n",
        "\n",
        "print(f\"AGI-SAC Framework ({FRAMEWORK_VERSION}) initialized.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGI-SAC Framework (AGI-SAC v1.0.0-alpha) initialized.\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "id": "fY_1etEYwgQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691b31cb-3660-4253-e10a-38979b682217"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uXZzmDQw_4G",
        "outputId": "3b1e810c-d60c-4698-bf53-6300f0b1b1fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ATcLUyfVJBa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/utils/__init__.py\n",
        "\"\"\"Utility classes and functions for the AGI-SAC framework.\"\"\"\n",
        "\n",
        "# Use absolute import since this is not run as part of a package installation\n",
        "from agisa_sac.utils.message_bus import MessageBus\n",
        "\n",
        "__all__ = [\"MessageBus\"]"
      ],
      "metadata": {
        "id": "yhXYUna_xTKn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "d42abbac-ec78-4bcd-a3a0-b520554ac4e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'agisa_sac'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-3961139606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Use absolute import since this is not run as part of a package installation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magisa_sac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_bus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessageBus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"MessageBus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agisa_sac'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/utils/message_bus.py\n",
        "import asyncio\n",
        "import time\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Optional, Callable, Any\n",
        "\n",
        "class MessageBus:\n",
        "    \"\"\"Simple asynchronous message passing system using a pub/sub pattern.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.subscribers: Dict[str, List[Callable]] = defaultdict(list)\n",
        "        self.message_history: List[Dict[str, Any]] = []\n",
        "        self._loop = None # Store loop for task creation if needed\n",
        "\n",
        "    def _get_loop(self):\n",
        "        \"\"\" Get the current asyncio event loop. \"\"\"\n",
        "        if self._loop is None:\n",
        "            try:\n",
        "                self._loop = asyncio.get_running_loop()\n",
        "            except RuntimeError:\n",
        "                # If no loop is running, create/get one (use case might be outside async context)\n",
        "                # This might have implications depending on how it's used.\n",
        "                # Consider warning or requiring explicit loop management.\n",
        "                warnings.warn(\"No running asyncio loop found. Getting/creating one.\", RuntimeWarning)\n",
        "                self._loop = asyncio.get_event_loop_policy().get_event_loop()\n",
        "        return self._loop\n",
        "\n",
        "    def subscribe(self, topic: str, callback: Callable):\n",
        "        \"\"\"Register a callback function for a specific topic.\"\"\"\n",
        "        if not callable(callback):\n",
        "            raise TypeError(\"Callback must be a callable function.\")\n",
        "        self.subscribers[topic].append(callback)\n",
        "        # print(f\"Subscribed {callback.__name__} to topic '{topic}'\") # Debug\n",
        "\n",
        "    def publish(self, topic: str, message: Dict):\n",
        "        \"\"\"Publish a message to all subscribers registered for the topic.\"\"\"\n",
        "        if not isinstance(message, dict):\n",
        "            warnings.warn(f\"Publishing non-dict message to '{topic}'. Converting to dict.\", RuntimeWarning)\n",
        "            message = {\"data\": message}\n",
        "\n",
        "        message['timestamp'] = time.time()\n",
        "        message['topic'] = topic\n",
        "        # Limit history size?\n",
        "        self.message_history.append(message)\n",
        "        if len(self.message_history) > 10000: # Example limit\n",
        "             self.message_history.pop(0)\n",
        "\n",
        "        # print(f\"Publishing to topic '{topic}': {message}\") # Debug\n",
        "        loop = self._get_loop()\n",
        "        for callback in self.subscribers[topic]:\n",
        "            try:\n",
        "                if asyncio.iscoroutinefunction(callback):\n",
        "                    # Create task to run async callback\n",
        "                    if loop.is_running():\n",
        "                         loop.create_task(self._execute_callback(callback, message.copy()))\n",
        "                    else:\n",
        "                         # If loop isn't running, might need different handling or warning\n",
        "                         warnings.warn(f\"Cannot schedule async callback {callback.__name__} for '{topic}' - loop not running.\", RuntimeWarning)\n",
        "                else:\n",
        "                    # Execute synchronous callback directly\n",
        "                    # Consider asyncio.to_thread if callback might block\n",
        "                    callback(message.copy())\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error executing callback {callback.__name__} for topic '{topic}': {e}\", RuntimeWarning)\n",
        "\n",
        "\n",
        "    async def _execute_callback(self, callback: Callable, message: Dict):\n",
        "        \"\"\"Safely execute an asynchronous callback.\"\"\"\n",
        "        try:\n",
        "            await callback(message)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Exception in async callback {callback.__name__}: {e}\", RuntimeWarning)\n",
        "\n",
        "    def get_recent_messages(self, topic: Optional[str] = None, limit: int = 10) -> List[Dict]:\n",
        "        \"\"\"Retrieve recent messages, optionally filtered by topic.\"\"\"\n",
        "        if topic:\n",
        "            # Iterate backwards for efficiency if history is large\n",
        "            filtered_messages = [m for m in reversed(self.message_history) if m['topic'] == topic]\n",
        "            return filtered_messages[:limit][::-1] # Get limit and reverse back\n",
        "        else:\n",
        "            return self.message_history[-limit:]\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"Clears the message history.\"\"\"\n",
        "        self.message_history = []\n",
        "\n",
        "    def clear_subscribers(self, topic: Optional[str] = None):\n",
        "        \"\"\"Clears subscribers, optionally for a specific topic.\"\"\"\n",
        "        if topic:\n",
        "            if topic in self.subscribers:\n",
        "                del self.subscribers[topic]\n",
        "        else:\n",
        "            self.subscribers.clear()\n",
        "\n"
      ],
      "metadata": {
        "id": "it1XdXHdxY4-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/components/__init__.py\n",
        "\"\"\"Core components defining agent internals and social structures.\"\"\"\n",
        "\n",
        "from .memory import MemoryEncapsulation, MemoryContinuumLayer\n",
        "from .cognitive import CognitiveDiversityEngine\n",
        "from .social import DynamicSocialGraph\n",
        "from .resonance import TemporalResonanceTracker, ResonanceLiturgy\n",
        "from .voice import VoiceEngine\n",
        "from .reflexivity import ReflexivityLayer\n",
        "\n",
        "__all__ = [\n",
        "    \"MemoryEncapsulation\",\n",
        "    \"MemoryContinuumLayer\",\n",
        "    \"CognitiveDiversityEngine\",\n",
        "    \"DynamicSocialGraph\",\n",
        "    \"TemporalResonanceTracker\",\n",
        "    \"ResonanceLiturgy\",\n",
        "    \"VoiceEngine\",\n",
        "    \"ReflexivityLayer\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "o1uFNd2hxfBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "d44d2837-da3f-4c91-8add-0cf10afb7d48"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "attempted relative import with no known parent package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-27517840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\"\"\"Core components defining agent internals and social structures.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemoryEncapsulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMemoryContinuumLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcognitive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCognitiveDiversityEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msocial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDynamicSocialGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/components/memory.py\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import hashlib\n",
        "import math\n",
        "import warnings\n",
        "import random\n",
        "from typing import Dict, List, Optional, Any\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dependency check for SentenceTransformer\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    HAS_SENTENCE_TRANSFORMER = True\n",
        "except ImportError:\n",
        "    HAS_SENTENCE_TRANSFORMER = False\n",
        "    # Warning is handled within MemoryContinuumLayer __init__\n",
        "\n",
        "# Import framework version (assuming it's accessible, e.g., from top-level __init__)\n",
        "try:\n",
        "    from .. import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "# Forward reference for MessageBus if needed for type hints\n",
        "from typing import TYPE_CHECKING\n",
        "if TYPE_CHECKING:\n",
        "    from ..utils.message_bus import MessageBus\n",
        "\n",
        "\n",
        "class MemoryEncapsulation:\n",
        "    \"\"\" Encapsulates a memory with state and methods. Includes serialization. \"\"\"\n",
        "    # --- Content from agisa_framework_serialization_v1 ---\n",
        "    def __init__(self, memory_id: str, content: Dict, importance: float = 0.5, confidence: float = 1.0,\n",
        "                 encoding_strength: float = 0.8, created_at: Optional[float] = None,\n",
        "                 last_accessed: Optional[float] = None, access_count: int = 0,\n",
        "                 embedding: Optional[np.ndarray] = None, theme: Optional[str] = None):\n",
        "        self.memory_id = memory_id\n",
        "        self.content = content\n",
        "        self.importance = np.clip(importance, 0.0, 1.0)\n",
        "        self.confidence = np.clip(confidence, 0.0, 1.0)\n",
        "        self.encoding_strength = np.clip(encoding_strength, 0.1, 1.0)\n",
        "        self.created_at = created_at if created_at is not None else time.time()\n",
        "        self.last_accessed = last_accessed if last_accessed is not None else self.created_at\n",
        "        self.access_count = access_count\n",
        "        self.verification_hash = self._generate_hash(content)\n",
        "        self.embedding = embedding\n",
        "        self.theme = theme if theme is not None else content.get(\"theme\", \"general\")\n",
        "\n",
        "    def access(self) -> Dict:\n",
        "        self.last_accessed = time.time()\n",
        "        self.access_count += 1\n",
        "        return self.content\n",
        "\n",
        "    def is_corrupted(self) -> bool:\n",
        "        return self._generate_hash(self.content) != self.verification_hash\n",
        "\n",
        "    def attempt_modification(self, new_content: Dict, external_influence: float) -> bool:\n",
        "        protection = (self.importance * 0.4 + self.encoding_strength * 0.6) * (1 - np.clip(external_influence, 0.0, 1.0))\n",
        "        if random.random() > protection:\n",
        "            self.content = new_content\n",
        "            self.verification_hash = self._generate_hash(new_content)\n",
        "            self.confidence *= 0.8\n",
        "            self.theme = new_content.get(\"theme\", self.theme)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def reinforce(self, strength_increase: float = 0.1):\n",
        "        self.encoding_strength = min(1.0, self.encoding_strength + strength_increase)\n",
        "\n",
        "    def decay(self, decay_rate: float = 0.05) -> float:\n",
        "        time_since_access = (time.time() - self.last_accessed) / 86400 # Days\n",
        "        decay_amount = decay_rate * time_since_access * (1 - self.importance * 0.5)\n",
        "        self.encoding_strength = max(0.1, self.encoding_strength - min(decay_amount, 0.2))\n",
        "        return decay_amount\n",
        "\n",
        "    def calculate_retrieval_strength(self) -> float:\n",
        "        recency = math.exp(-0.1 * (time.time() - self.last_accessed) / 86400)\n",
        "        return (recency * 0.3 + self.importance * 0.3 + self.encoding_strength * 0.4)\n",
        "\n",
        "    def set_embedding(self, embedding: np.ndarray):\n",
        "        self.embedding = embedding\n",
        "\n",
        "    def _generate_hash(self, content: Dict) -> str:\n",
        "        try: content_string = json.dumps(content, sort_keys=True).encode()\n",
        "        except TypeError: content_string = str(content).encode()\n",
        "        return hashlib.md5(content_string).hexdigest()\n",
        "\n",
        "    def to_dict(self, include_embedding: bool = False) -> Dict:\n",
        "        state = { 'memory_id': self.memory_id, 'content': self.content, 'theme': self.theme, 'importance': self.importance,\n",
        "                  'confidence': self.confidence, 'encoding_strength': self.encoding_strength, 'created_at': self.created_at,\n",
        "                  'last_accessed': self.last_accessed, 'access_count': self.access_count, 'verification_hash': self.verification_hash }\n",
        "        if include_embedding and self.embedding is not None:\n",
        "            state['embedding'] = self.embedding.tolist()\n",
        "        return state\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any]) -> 'MemoryEncapsulation':\n",
        "        embedding = np.array(data['embedding']) if 'embedding' in data and data['embedding'] is not None else None\n",
        "        instance = cls( memory_id=data['memory_id'], content=data['content'], importance=data.get('importance', 0.5),\n",
        "                        confidence=data.get('confidence', 1.0), encoding_strength=data.get('encoding_strength', 0.8),\n",
        "                        created_at=data.get('created_at'), last_accessed=data.get('last_accessed'),\n",
        "                        access_count=data.get('access_count', 0), embedding=embedding, theme=data.get('theme') )\n",
        "        loaded_hash = data.get('verification_hash')\n",
        "        if loaded_hash and instance._generate_hash(instance.content) != loaded_hash:\n",
        "             warnings.warn(f\"Memory {instance.memory_id}: Hash mismatch on load.\", RuntimeWarning)\n",
        "             instance.confidence *= 0.5\n",
        "        return instance\n",
        "\n",
        "\n",
        "class MemoryContinuumLayer:\n",
        "    \"\"\" Enhanced memory system for an agent. Includes serialization. \"\"\"\n",
        "    # --- Content from agisa_framework_serialization_v1 ---\n",
        "    def __init__(self, agent_id: str, capacity: int = 100,\n",
        "                 use_semantic: bool = True, message_bus: Optional['MessageBus'] = None): # Use forward ref string\n",
        "        self.agent_id = agent_id\n",
        "        self.capacity = capacity\n",
        "        self.use_semantic = use_semantic and HAS_SENTENCE_TRANSFORMER\n",
        "        self.message_bus = message_bus\n",
        "        self.memories: Dict[str, MemoryEncapsulation] = {}\n",
        "        self.memory_indices = {\"term\": defaultdict(list)}\n",
        "        self.last_update = time.time()\n",
        "        self.encoder = None\n",
        "        if self.use_semantic: self._initialize_encoder()\n",
        "\n",
        "    def _initialize_encoder(self):\n",
        "        if self.use_semantic and self.encoder is None:\n",
        "            try: self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            except Exception as e: warnings.warn(f\"Agent {self.agent_id}: No ST model: {e}. Semantic off.\", RuntimeWarning); self.use_semantic = False\n",
        "\n",
        "    def add_memory(self, content: Dict, importance: float = 0.5) -> str:\n",
        "        if self.use_semantic and self.encoder is None: self._initialize_encoder()\n",
        "        memory_id = f\"mem_{self.agent_id}_{int(time.time())}_{random.randint(1000, 9999)}\"\n",
        "        memory = MemoryEncapsulation(memory_id, content, importance)\n",
        "        if self.use_semantic and self.encoder:\n",
        "            try: content_str = json.dumps(content, sort_keys=True); embedding = self.encoder.encode([content_str])[0]; memory.set_embedding(embedding)\n",
        "            except Exception as e: warnings.warn(f\"Agent {self.agent_id}: Mem encode fail {memory_id}: {e}\", RuntimeWarning)\n",
        "        self.memories[memory_id] = memory; self._update_indices(memory_id, content)\n",
        "        if len(self.memories) > self.capacity: self._remove_weakest_memory()\n",
        "        if self.message_bus: self.message_bus.publish('memory_added', {'agent_id': self.agent_id, 'memory_id': memory_id, 'importance': importance, 'theme': memory.theme})\n",
        "        return memory_id\n",
        "\n",
        "    def retrieve_memory(self, query: str, threshold: float = 0.3, limit: int = 10) -> List[Dict]:\n",
        "        if self.use_semantic and self.encoder is None: self._initialize_encoder()\n",
        "        matches = {} # Code combines term and semantic search results\n",
        "        # Term search\n",
        "        query_terms = set(query.lower().split()); term_relevance_scores = defaultdict(float)\n",
        "        if query_terms:\n",
        "            for term in query_terms:\n",
        "                for memory_id in self.memory_indices[\"term\"].get(term, []): term_relevance_scores[memory_id] += 0.1\n",
        "            for memory_id, term_relevance in term_relevance_scores.items():\n",
        "                if memory_id in self.memories:\n",
        "                    memory = self.memories[memory_id]; score = memory.calculate_retrieval_strength() * (1 + term_relevance)\n",
        "                    if score >= threshold: match_data = memory.to_dict(); match_data[\"relevance_score\"] = score; match_data[\"match_type\"] = \"term\"; matches[memory_id] = match_data\n",
        "        # Semantic search\n",
        "        if self.use_semantic and self.encoder and query.strip():\n",
        "            try:\n",
        "                query_embedding = self.encoder.encode([query])[0]; query_norm = np.linalg.norm(query_embedding)\n",
        "                if query_norm > 1e-6:\n",
        "                    mem_ids = [mid for mid, mem in self.memories.items() if mem.embedding is not None];\n",
        "                    if mem_ids:\n",
        "                        mem_embeddings = np.array([self.memories[mid].embedding for mid in mem_ids]); mem_norms = np.linalg.norm(mem_embeddings, axis=1); valid_indices = mem_norms > 1e-6\n",
        "                        if np.any(valid_indices):\n",
        "                            mem_embeddings_valid = mem_embeddings[valid_indices]; mem_norms_valid = mem_norms[valid_indices]; mem_ids_valid = np.array(mem_ids)[valid_indices]\n",
        "                            similarities = np.dot(mem_embeddings_valid, query_embedding) / (mem_norms_valid * query_norm)\n",
        "                            for i, memory_id in enumerate(mem_ids_valid):\n",
        "                                similarity = similarities[i]\n",
        "                                if similarity >= threshold:\n",
        "                                    memory = self.memories[memory_id]; score = memory.calculate_retrieval_strength() * similarity; match_type = \"semantic\"\n",
        "                                    if memory_id in matches:\n",
        "                                        if score > matches[memory_id][\"relevance_score\"]: matches[memory_id][\"relevance_score\"] = score; matches[memory_id][\"match_type\"] = \"hybrid\"\n",
        "                                    else: match_data = memory.to_dict(); match_data[\"relevance_score\"] = score; match_data[\"match_type\"] = match_type; matches[memory_id] = match_data\n",
        "            except Exception as e: warnings.warn(f\"Agent {self.agent_id}: Semantic fail query '{query}': {e}\", RuntimeWarning)\n",
        "        # Combine and finalize\n",
        "        sorted_matches = sorted(matches.values(), key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "        for match in sorted_matches[:limit]:\n",
        "             if match[\"memory_id\"] in self.memories: self.memories[match[\"memory_id\"]].access()\n",
        "        return sorted_matches[:limit]\n",
        "\n",
        "    def update_all_memories(self):\n",
        "        removed_count = 0; corrupted_count = 0; memory_ids_to_remove = []\n",
        "        for memory_id, memory in list(self.memories.items()):\n",
        "            memory.decay();\n",
        "            if memory.is_corrupted(): corrupted_count += 1; memory_ids_to_remove.append(memory_id); continue\n",
        "            if memory.encoding_strength < 0.15: memory_ids_to_remove.append(memory_id)\n",
        "        for memory_id in memory_ids_to_remove:\n",
        "            if self._remove_memory(memory_id): removed_count += 1\n",
        "        self.last_update = time.time()\n",
        "        if self.message_bus and (removed_count > 0 or corrupted_count > 0): self.message_bus.publish('memory_maintenance', {'agent_id': self.agent_id, 'removed': removed_count, 'corrupted': corrupted_count, 'remain': len(self.memories)})\n",
        "\n",
        "    def reinforce_memory(self, memory_id: str, strength: float = 0.1) -> bool:\n",
        "        if memory_id in self.memories: self.memories[memory_id].reinforce(strength); return True\n",
        "        return False\n",
        "    def get_memory_by_id(self, memory_id: str) -> Optional[Dict]:\n",
        "        if memory_id in self.memories: memory = self.memories[memory_id]; memory.access(); return memory.to_dict()\n",
        "        return None\n",
        "    def link_memories(self, source_id: str, target_id: str, link_type: str = \"related\") -> bool:\n",
        "        if source_id in self.memories and target_id in self.memories:\n",
        "            source_memory = self.memories[source_id]; source_content = source_memory.content\n",
        "            if \"links\" not in source_content: source_content[\"links\"] = []\n",
        "            link_exists = any(link.get(\"target_id\") == target_id for link in source_content.get(\"links\", []))\n",
        "            if link_exists: return False\n",
        "            source_content[\"links\"].append({\"target_id\": target_id, \"link_type\": link_type, \"created_at\": time.time()})\n",
        "            source_memory.content = source_content; source_memory.verification_hash = source_memory._generate_hash(source_content)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def _update_indices(self, memory_id: str, content: Dict):\n",
        "        text_to_index = \"\"\n",
        "        def extract_strings(item): nonlocal text_to_index; [extract_strings(v) for v in item.values()] if isinstance(item, dict) else [extract_strings(v) for v in item] if isinstance(item, list) else None; text_to_index += item + \" \" if isinstance(item, str) else \"\"\n",
        "        extract_strings(content); terms = set(text_to_index.lower().split())\n",
        "        for term in terms:\n",
        "            if memory_id not in self.memory_indices[\"term\"][term]: self.memory_indices[\"term\"][term].append(memory_id)\n",
        "\n",
        "    def _remove_memory(self, memory_id: str) -> bool:\n",
        "        if memory_id in self.memories:\n",
        "            content = self.memories[memory_id].content; text_to_index = \"\"\n",
        "            def extract_strings(item): nonlocal text_to_index; [extract_strings(v) for v in item.values()] if isinstance(item, dict) else [extract_strings(v) for v in item] if isinstance(item, list) else None; text_to_index += item + \" \" if isinstance(item, str) else \"\"\n",
        "            extract_strings(content); terms = set(text_to_index.lower().split())\n",
        "            for term in terms:\n",
        "                if term in self.memory_indices[\"term\"]:\n",
        "                    if memory_id in self.memory_indices[\"term\"][term]: self.memory_indices[\"term\"][term].remove(memory_id)\n",
        "                    if not self.memory_indices[\"term\"][term]: del self.memory_indices[\"term\"][term]\n",
        "            del self.memories[memory_id]; return True\n",
        "        return False\n",
        "\n",
        "    def _remove_weakest_memory(self):\n",
        "        if not self.memories: return\n",
        "        try: weakest_id = min(self.memories, key=lambda mid: self.memories[mid].calculate_retrieval_strength()); self._remove_memory(weakest_id)\n",
        "        except ValueError: warnings.warn(f\"Agent {self.agent_id}: Weakest memory fail.\", RuntimeWarning)\n",
        "\n",
        "    def get_current_focus_theme(self) -> str:\n",
        "        latest_focus_mem = None; latest_ts = 0\n",
        "        for mem in self.memories.values():\n",
        "            if mem.content.get(\"type\") == \"current_focus\" and mem.created_at > latest_ts: latest_focus_mem = mem; latest_ts = mem.created_at\n",
        "        if latest_focus_mem: return latest_focus_mem.theme\n",
        "        if self.memories:\n",
        "             try: latest_mem_id = max(self.memories, key=lambda mid: self.memories[mid].created_at); return self.memories[latest_mem_id].theme\n",
        "             except ValueError: return \"general\"\n",
        "        return \"general\"\n",
        "\n",
        "    def _rebuild_indices(self):\n",
        "        self.memory_indices = {\"term\": defaultdict(list)}\n",
        "        # print(f\"Agent {self.agent_id}: Rebuilding memory indices...\") # Verbose\n",
        "        start_time = time.time()\n",
        "        for memory_id, memory in self.memories.items(): self._update_indices(memory_id, memory.content)\n",
        "        duration = time.time() - start_time\n",
        "        # print(f\"Agent {self.agent_id}: Index rebuild complete [{duration:.2f}s].\") # Verbose\n",
        "\n",
        "    def to_dict(self, include_embeddings: bool = False) -> Dict:\n",
        "        return { \"version\": FRAMEWORK_VERSION, \"agent_id\": self.agent_id, \"capacity\": self.capacity,\n",
        "                 \"use_semantic_config\": self.use_semantic, \"last_update\": self.last_update,\n",
        "                 \"memories\": {mid: mem.to_dict(include_embedding=include_embeddings) for mid, mem in self.memories.items()} }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], message_bus: Optional['MessageBus'] = None) -> 'MemoryContinuumLayer':\n",
        "        loaded_version = data.get(\"version\"); agent_id = data['agent_id']\n",
        "        if loaded_version != FRAMEWORK_VERSION: warnings.warn(f\"Agent {agent_id}: Loading memory v '{loaded_version}' into v '{FRAMEWORK_VERSION}'.\", UserWarning)\n",
        "        instance = cls( agent_id=agent_id, capacity=data.get('capacity', 100), use_semantic=data.get('use_semantic_config', True), message_bus=message_bus )\n",
        "        instance.last_update = data.get('last_update', time.time()); instance.memories = {}\n",
        "        memories_data = data.get(\"memories\", {}); corrupted_on_load = 0\n",
        "        for mid, mem_data in memories_data.items():\n",
        "            try:\n",
        "                mem_instance = MemoryEncapsulation.from_dict(mem_data); instance.memories[mid] = mem_instance\n",
        "                # Optional immediate hash check\n",
        "                # loaded_hash = mem_data.get('verification_hash'); if loaded_hash and mem_instance._generate_hash(mem_instance.content) != loaded_hash: corrupted_on_load += 1; warnings.warn(f\"Mem {mid} hash mismatch.\", RuntimeWarning)\n",
        "            except Exception as e: warnings.warn(f\"Failed load mem {mid} for {agent_id}: {e}\", RuntimeWarning)\n",
        "        instance._rebuild_indices() # Rebuild after loading all\n",
        "        # print(f\"Agent {agent_id}: Mem layer reconstruct. {len(instance.memories)} mems. {corrupted_on_load} hash mismatches.\") # Verbose\n",
        "        return instance\n",
        "\n"
      ],
      "metadata": {
        "id": "4thRVIUyxjhi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/components/voice.py\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Dict, Optional, Any\n",
        "\n",
        "# Import framework version\n",
        "try:\n",
        "    from .. import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "class VoiceEngine:\n",
        "    \"\"\" Agent's voice/style engine. Includes serialization. \"\"\"\n",
        "    def __init__(self, agent_id: str, initial_style: Optional[Dict] = None):\n",
        "        self.agent_id = agent_id\n",
        "        # Default linguistic signature\n",
        "        self.linguistic_signature = {\n",
        "            \"style_vector\": np.random.rand(64) * 0.5 + 0.25, # Example dimension\n",
        "            \"archetype\": \"neutral\",\n",
        "            \"sentence_structure\": \"declarative\",\n",
        "            \"vocabulary_richness\": 0.5\n",
        "        }\n",
        "        if initial_style:\n",
        "            # Validate and update with initial style if provided\n",
        "            if 'style_vector' in initial_style and isinstance(initial_style['style_vector'], list):\n",
        "                 initial_style['style_vector'] = np.array(initial_style['style_vector'])\n",
        "            self.linguistic_signature.update(initial_style)\n",
        "\n",
        "    def generate_response(self, prompt: str) -> str:\n",
        "        \"\"\" Generates a stylized response based on the prompt and signature. (Placeholder) \"\"\"\n",
        "        style = self.linguistic_signature.get('archetype', 'unknown')\n",
        "        structure = self.linguistic_signature.get('sentence_structure', 'simple')\n",
        "        # Extract context (e.g., last relevant line of prompt)\n",
        "        context_lines = [line.strip() for line in prompt.strip().splitlines() if line.strip()]\n",
        "        context = context_lines[-1] if context_lines else \"prompt\"\n",
        "        return f\"[{style}/{structure}] Response to: {context[:60]}...\"\n",
        "\n",
        "    def evolve_style(self, influence: Dict):\n",
        "        \"\"\" Evolves the linguistic signature based on external influence. \"\"\"\n",
        "        # print(f\"Agent {self.agent_id} voice style evolving with influence: {influence}\") # Verbose\n",
        "        if \"archetype\" in influence and isinstance(influence['archetype'], str):\n",
        "            self.linguistic_signature[\"archetype\"] = influence[\"archetype\"]\n",
        "        if \"sentence_structure\" in influence and isinstance(influence['sentence_structure'], str):\n",
        "            self.linguistic_signature[\"sentence_structure\"] = influence[\"sentence_structure\"]\n",
        "        if \"vocabulary_richness\" in influence and isinstance(influence['vocabulary_richness'], (int, float)):\n",
        "             self.linguistic_signature[\"vocabulary_richness\"] = np.clip(influence['vocabulary_richness'], 0.0, 1.0)\n",
        "\n",
        "        # Apply shift to style vector based on influence type or magnitude\n",
        "        shift_magnitude = influence.get(\"shift_magnitude\", 0.1)\n",
        "        if influence.get(\"archetype\") == \"enlightened\": shift_magnitude = 0.2 # Example specific shift\n",
        "\n",
        "        if isinstance(self.linguistic_signature[\"style_vector\"], np.ndarray):\n",
        "            noise = (np.random.rand(*self.linguistic_signature[\"style_vector\"].shape) - 0.5) * shift_magnitude\n",
        "            self.linguistic_signature[\"style_vector\"] += noise\n",
        "            # Optional: Normalize or clip the vector to prevent unbounded growth\n",
        "            # norm = np.linalg.norm(self.linguistic_signature[\"style_vector\"])\n",
        "            # if norm > 1.0: self.linguistic_signature[\"style_vector\"] /= norm\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\" Serializes the voice engine state. \"\"\"\n",
        "        sig = self.linguistic_signature.copy()\n",
        "        if 'style_vector' in sig and isinstance(sig['style_vector'], np.ndarray):\n",
        "            sig['style_vector'] = sig['style_vector'].tolist() # Convert numpy array\n",
        "        return {\n",
        "            \"version\": FRAMEWORK_VERSION,\n",
        "            \"linguistic_signature\": sig\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], agent_id: str) -> 'VoiceEngine':\n",
        "        \"\"\" Reconstructs the voice engine from serialized data. \"\"\"\n",
        "        loaded_version = data.get(\"version\")\n",
        "        if loaded_version != FRAMEWORK_VERSION:\n",
        "            warnings.warn(f\"Agent {agent_id}: Loading voice state v '{loaded_version}' into v '{FRAMEWORK_VERSION}'.\", UserWarning)\n",
        "\n",
        "        instance = cls(agent_id=agent_id) # Basic init\n",
        "        sig_data = data.get(\"linguistic_signature\", {})\n",
        "        if 'style_vector' in sig_data and isinstance(sig_data['style_vector'], list):\n",
        "             # Ensure loaded vector has correct shape if needed\n",
        "             try:\n",
        "                 loaded_vector = np.array(sig_data['style_vector'])\n",
        "                 # Check shape against default if necessary, e.g., expected_shape = (64,)\n",
        "                 # if loaded_vector.shape != expected_shape: ... handle error ...\n",
        "                 sig_data['style_vector'] = loaded_vector\n",
        "             except Exception as e:\n",
        "                 warnings.warn(f\"Agent {agent_id}: Failed to load style vector: {e}. Using default.\", RuntimeWarning)\n",
        "                 sig_data['style_vector'] = instance.linguistic_signature['style_vector'] # Fallback\n",
        "\n",
        "        # Update the instance's signature, preserving defaults if keys are missing\n",
        "        instance.linguistic_signature.update(sig_data)\n",
        "        return instance\n"
      ],
      "metadata": {
        "id": "rCgepgiwxnBR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/components/cognitive.py\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import time\n",
        "from typing import Dict, List, Optional, Any, TYPE_CHECKING\n",
        "\n",
        "# Relative imports for type hints\n",
        "if TYPE_CHECKING:\n",
        "    from .memory import MemoryContinuumLayer\n",
        "    from ..utils.message_bus import MessageBus\n",
        "\n",
        "# Import framework version\n",
        "try:\n",
        "    from .. import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "class CognitiveDiversityEngine:\n",
        "    \"\"\" Agent's decision-making engine. Includes serialization. \"\"\"\n",
        "    def __init__(self, agent_id: str, personality: Dict, memory_layer: 'MemoryContinuumLayer',\n",
        "                 message_bus: Optional['MessageBus'] = None):\n",
        "        self.agent_id = agent_id\n",
        "        self.personality = {k: np.clip(v, 0.0, 1.0) for k, v in personality.items()}\n",
        "        self.memory_layer = memory_layer # Keep reference\n",
        "        self.message_bus = message_bus   # Keep reference\n",
        "        # State\n",
        "        self.heuristics = np.random.rand(4, 4) * 0.5 + 0.25 # Shape (State Aspects, Decision Approaches)\n",
        "        self.learning_rate = 0.05\n",
        "        self.stability_factor = 0.3\n",
        "        self.cognitive_state = np.ones(4) / 4 # Vector over state aspects\n",
        "        self.decision_history: List[Dict] = [] # Runtime history\n",
        "\n",
        "    def update_heuristics(self, situational_entropy: float):\n",
        "        \"\"\" Updates the agent's decision heuristics based on recent experience and state. \"\"\"\n",
        "        memories = self.memory_layer.retrieve_memory(\"decision outcome context\", limit=5, threshold=0.2)\n",
        "        salience = sum(m[\"importance\"] * m[\"confidence\"] for m in memories) / len(memories) if memories else 0.1\n",
        "        salience = np.clip(salience, 0.0, 1.0)\n",
        "        personality_vector = np.array([\n",
        "            self.personality.get(\"curiosity\", 0.5),\n",
        "            self.personality.get(\"conformity\", 0.5),\n",
        "            self.personality.get(\"openness\", 0.5),\n",
        "            self.personality.get(\"consistency\", 0.5)\n",
        "        ])\n",
        "        # Introduce a change based on salience, entropy, and personality\n",
        "        d_heuristics = salience * situational_entropy * personality_vector.reshape(-1, 1) * (np.random.rand(4,4) - 0.5) * 0.5\n",
        "        # Apply update with stability\n",
        "        self.heuristics += self.learning_rate * (d_heuristics - self.stability_factor * (self.heuristics - 0.5))\n",
        "        # Sigmoid-like activation to keep heuristics in a reasonable range (0-1)\n",
        "        self.heuristics = 1 / (1 + np.exp(-self.heuristics))\n",
        "        # Clip to prevent extreme values, maintaining exploration possibility\n",
        "        self.heuristics = np.clip(self.heuristics, 0.1, 0.9)\n",
        "\n",
        "        if self.message_bus:\n",
        "            self.message_bus.publish('cognitive_heuristic_update', {\n",
        "                'agent_id': self.agent_id,\n",
        "                'magnitude': float(np.mean(np.abs(d_heuristics))),\n",
        "                'entropy': float(situational_entropy),\n",
        "                'salience': float(salience)\n",
        "            })\n",
        "\n",
        "    def decide(self, query: str, peer_influence: Dict[str, float]) -> str:\n",
        "        \"\"\" Makes a decision based on memory, peer influence, and heuristics. \"\"\"\n",
        "        # Incorporate memory relevance and peer influence into cognitive state\n",
        "        memories = self.memory_layer.retrieve_memory(query, limit=5, threshold=0.25)\n",
        "        memory_weight = sum(m[\"relevance_score\"] * m[\"confidence\"] for m in memories) / len(memories) if memories else 0.0\n",
        "        memory_weight = np.clip(memory_weight, 0.0, 1.0)\n",
        "\n",
        "        total_influence = sum(peer_influence.values())\n",
        "        normalized_influence = {pid: w / total_influence for pid, w in peer_influence.items()} if total_influence > 1e-6 else {}\n",
        "        peer_weight = sum(normalized_influence.values())\n",
        "        peer_weight = np.clip(peer_weight, 0.0, 1.0)\n",
        "\n",
        "        # Decay cognitive state slightly\n",
        "        self.cognitive_state *= (1 - 0.1) # Decay factor\n",
        "\n",
        "        # Update cognitive state based on inputs\n",
        "        if memories:\n",
        "            memory_state_influence = np.zeros(4)\n",
        "            total_mem_relevance = sum(m['relevance_score'] for m in memories)\n",
        "            if total_mem_relevance > 1e-6:\n",
        "                 # Example mapping: High relevance/confidence -> aspects 1 & 4, Low -> 0 & 2\n",
        "                 for m in memories:\n",
        "                    mem_impact = m['relevance_score'] / total_mem_relevance\n",
        "                    memory_state_influence[3] += mem_impact * m['importance'] # Maps to state aspect 3 (e.g., Security/Consistency)\n",
        "                    memory_state_influence[1] += mem_impact * m['confidence'] # Maps to state aspect 1 (e.g., Rationality/Systematic)\n",
        "                    memory_state_influence[2] += mem_impact * (1 - m['importance']) # Maps to state aspect 2 (e.g., Novelty/Creativity)\n",
        "                    memory_state_influence[0] += mem_impact * (1 - m['confidence']) # Maps to state aspect 0 (e.g., Emotional/Impulsive)\n",
        "\n",
        "                 # Normalize influence if non-zero\n",
        "                 if np.sum(memory_state_influence) > 1e-6:\n",
        "                     memory_state_influence /= np.sum(memory_state_influence)\n",
        "\n",
        "                 self.cognitive_state += 0.1 * memory_weight * memory_state_influence # Add memory influence\n",
        "\n",
        "        if normalized_influence:\n",
        "            # Example fixed influence vector for peer pressure: biases towards conformity/rationality\n",
        "            peer_state_influence = np.array([0.0, 0.6, 0.0, 0.4])\n",
        "            self.cognitive_state += 0.1 * peer_weight * peer_state_influence # Add peer influence\n",
        "\n",
        "        # Re-normalize cognitive state vector\n",
        "        if np.sum(self.cognitive_state) > 1e-6:\n",
        "            self.cognitive_state /= np.sum(self.cognitive_state)\n",
        "        else:\n",
        "            self.cognitive_state = np.ones(4) / 4 # Reset if state becomes zero\n",
        "\n",
        "        # Calculate decision probabilities based on current cognitive state and heuristics\n",
        "        decision_probs = np.dot(self.cognitive_state, self.heuristics)\n",
        "\n",
        "        # Normalize decision probabilities\n",
        "        if np.sum(decision_probs) > 1e-6:\n",
        "            decision_probs /= np.sum(decision_probs)\n",
        "        else:\n",
        "            decision_probs = np.ones(4) / 4 # Equal probability if sum is zero\n",
        "\n",
        "        # Select decision approach (with some exploration based on personality)\n",
        "        options = [\"Approach A: Systematic\", \"Approach B: Creative\", \"Approach C: Balanced\", \"Approach D: Efficient\"]\n",
        "        exploration_prob = 0.1 + 0.3 * self.personality.get(\"openness\", 0.5) # Higher openness -> more exploration\n",
        "\n",
        "        if random.random() > exploration_prob:\n",
        "            # Exploitation: Choose the highest probability\n",
        "            choice_idx = np.argmax(decision_probs)\n",
        "        else:\n",
        "            # Exploration: Sample based on probabilities\n",
        "            try:\n",
        "                 choice_idx = np.random.choice(len(options), p=decision_probs)\n",
        "            except ValueError:\n",
        "                 # Fallback if probabilities don't sum to 1 due to floating point issues\n",
        "                 choice_idx = np.random.choice(len(options))\n",
        "\n",
        "        response = options[choice_idx]\n",
        "\n",
        "        # Record decision for history and potential future learning\n",
        "        decision_record = {\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"cognitive_state\": self.cognitive_state.tolist(), # Store as list for serialization\n",
        "            \"decision_probs\": decision_probs.tolist(), # Store as list\n",
        "            \"memory_weight\": memory_weight,\n",
        "            \"peer_weight\": peer_weight,\n",
        "            \"exploration_used\": random.random() <= exploration_prob, # Was exploration used?\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "        self.decision_history.append(decision_record)\n",
        "        self.decision_history = self.decision_history[-100:] # Limit history size\n",
        "\n",
        "        # Add memory about the decision context (theme based on current focus)\n",
        "        memory_content = {\n",
        "            \"type\": \"decision_context\",\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"cognitive_state_at_decision\": self.cognitive_state.tolist(),\n",
        "            \"theme\": self.memory_layer.get_current_focus_theme(),\n",
        "            \"timestamp\": decision_record[\"timestamp\"]\n",
        "        }\n",
        "        self.memory_layer.add_memory(memory_content, importance=0.6) # Decisions are moderately important\n",
        "\n",
        "        if self.message_bus:\n",
        "            self.message_bus.publish('agent_decision', {\n",
        "                'agent_id': self.agent_id,\n",
        "                'query': query,\n",
        "                'response': response,\n",
        "                'decision_probs': decision_probs.tolist(),\n",
        "                'cognitive_state': self.cognitive_state.tolist()\n",
        "            })\n",
        "\n",
        "        return response\n",
        "\n",
        "    def learn_from_feedback(self, decision_index: int, reward: float):\n",
        "        \"\"\" Updates heuristics based on feedback for a specific past decision. \"\"\"\n",
        "        if 0 <= decision_index < len(self.decision_history):\n",
        "            decision = self.decision_history[decision_index]\n",
        "            response = decision[\"response\"]\n",
        "            cognitive_state_at_decision = np.array(decision[\"cognitive_state\"])\n",
        "\n",
        "            try:\n",
        "                # Map the response string back to the option index\n",
        "                options = [\"Approach A: Systematic\", \"Approach B: Creative\", \"Approach C: Balanced\", \"Approach D: Efficient\"]\n",
        "                choice_idx = options.index(response)\n",
        "\n",
        "                # Update the heuristics related to the state and chosen approach\n",
        "                # The update is proportional to the cognitive state at the time of decision and the reward\n",
        "                update_vector = cognitive_state_at_decision * reward * self.learning_rate * 0.5 # Factor of 0.5 to dampen updates\n",
        "                self.heuristics[:, choice_idx] += update_vector\n",
        "\n",
        "                # Re-apply activation and clipping\n",
        "                self.heuristics = 1 / (1 + np.exp(-self.heuristics))\n",
        "                self.heuristics = np.clip(self.heuristics, 0.1, 0.9)\n",
        "\n",
        "                if self.message_bus:\n",
        "                    self.message_bus.publish('agent_feedback_learning', {\n",
        "                        'agent_id': self.agent_id,\n",
        "                        'decision_index': decision_index,\n",
        "                        'reward': reward,\n",
        "                        'magnitude': float(np.sum(np.abs(update_vector))) # Magnitude of change\n",
        "                    })\n",
        "\n",
        "                return True\n",
        "            except ValueError:\n",
        "                warnings.warn(f\"Agent {self.agent_id}: Response '{response}' not a recognized decision approach.\", RuntimeWarning)\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Agent {self.agent_id}: Error applying feedback for decision {decision_index}: {e}\", RuntimeWarning)\n",
        "        else:\n",
        "            warnings.warn(f\"Agent {self.agent_id}: Invalid decision index {decision_index}.\", RuntimeWarning)\n",
        "        return False\n",
        "\n",
        "    def to_dict(self, history_limit: int = 10) -> Dict:\n",
        "        \"\"\" Serializes the cognitive engine state. \"\"\"\n",
        "        return {\n",
        "            \"version\": FRAMEWORK_VERSION,\n",
        "            \"personality\": self.personality,\n",
        "            \"heuristics\": self.heuristics.tolist(), # Convert numpy array\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"stability_factor\": self.stability_factor,\n",
        "            \"cognitive_state\": self.cognitive_state.tolist(), # Convert numpy array\n",
        "            \"decision_history_summary\": self.decision_history[-history_limit:] # Only save recent history summary\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], agent_id: str, memory_layer: 'MemoryContinuumLayer', message_bus: Optional['MessageBus']) -> 'CognitiveDiversityEngine':\n",
        "        \"\"\" Reconstructs the cognitive engine from serialized data. \"\"\"\n",
        "        loaded_version = data.get(\"version\")\n",
        "        if loaded_version != FRAMEWORK_VERSION:\n",
        "            warnings.warn(f\"Agent {agent_id}: Loading cognitive state v '{loaded_version}' into v '{FRAMEWORK_VERSION}'.\", UserWarning)\n",
        "\n",
        "        # Need to pass memory_layer and message_bus which are runtime objects\n",
        "        instance = cls(\n",
        "            agent_id=agent_id,\n",
        "            personality=data['personality'], # Load personality\n",
        "            memory_layer=memory_layer,\n",
        "            message_bus=message_bus\n",
        "        )\n",
        "\n",
        "        # Load state variables, providing defaults if keys are missing\n",
        "        instance.heuristics = np.array(data.get('heuristics', instance.heuristics))\n",
        "        instance.learning_rate = data.get('learning_rate', instance.learning_rate)\n",
        "        instance.stability_factor = data.get('stability_factor', instance.stability_factor)\n",
        "        instance.cognitive_state = np.array(data.get('cognitive_state', instance.cognitive_state))\n",
        "\n",
        "        # Decision history summary is loaded for info only, don't overwrite runtime history\n",
        "        # instance.decision_history = data.get('decision_history_summary', [])\n",
        "\n",
        "        return instance"
      ],
      "metadata": {
        "id": "TmBCNsm6xqnN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/components/social.py\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "from scipy.sparse import lil_matrix, csr_matrix, coo_matrix\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Optional, Tuple, Set, Any\n",
        "\n",
        "# Dependency checks\n",
        "try:\n",
        "    import cupy as cp\n",
        "    HAS_CUPY = True\n",
        "except ImportError:\n",
        "    HAS_CUPY = False\n",
        "try:\n",
        "    import community as community_louvain\n",
        "    HAS_LOUVAIN = True\n",
        "except ImportError:\n",
        "    HAS_LOUVAIN = False\n",
        "\n",
        "# Import framework version\n",
        "try:\n",
        "    from .. import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "# Forward reference for MessageBus\n",
        "from typing import TYPE_CHECKING\n",
        "if TYPE_CHECKING:\n",
        "    from ..utils.message_bus import MessageBus\n",
        "\n",
        "\n",
        "class DynamicSocialGraph:\n",
        "    \"\"\" Manages dynamic influence network. Includes serialization. \"\"\"\n",
        "    def __init__(self, num_agents: int, agent_ids: List[str], use_gpu: bool = False, message_bus: Optional['MessageBus'] = None): # Add message_bus\n",
        "        self.num_agents = num_agents\n",
        "        self.agent_ids = agent_ids\n",
        "        self.id_to_index = {agent_id: i for i, agent_id in enumerate(self.agent_ids)}\n",
        "        self.use_gpu = use_gpu and HAS_CUPY\n",
        "        self.message_bus = message_bus # Store reference\n",
        "        # State\n",
        "        self.influence_matrix = lil_matrix((num_agents, num_agents), dtype=np.float32)\n",
        "        for i in range(num_agents):\n",
        "            for j in range(num_agents):\n",
        "                if i != j: self.influence_matrix[i, j] = random.uniform(0.05, 0.2)\n",
        "        self.reputation = np.ones(num_agents, dtype=np.float32)\n",
        "        self._convert_to_csr()\n",
        "        self.influence_matrix_gpu = None\n",
        "        self.reputation_gpu = None\n",
        "        if self.use_gpu: self._transfer_to_gpu()\n",
        "        self.edge_changes_since_last_community_check = 0\n",
        "        self.last_communities: Optional[List[Set[str]]] = None # Store as set of agent IDs\n",
        "\n",
        "    def _convert_to_csr(self): self.influence_matrix_csr = self.influence_matrix.tocsr()\n",
        "    def _transfer_to_gpu(self):\n",
        "        if HAS_CUPY:\n",
        "            try:\n",
        "                self.influence_matrix_gpu = cp.sparse.csr_matrix((cp.array(self.influence_matrix_csr.data), cp.array(self.influence_matrix_csr.indices), cp.array(self.influence_matrix_csr.indptr)), shape=self.influence_matrix_csr.shape, dtype=cp.float32)\n",
        "                self.reputation_gpu = cp.array(self.reputation, dtype=cp.float32)\n",
        "            except Exception as e: warnings.warn(f\"GPU transfer fail: {e}. CPU fallback.\", RuntimeWarning); self.use_gpu = False; self.influence_matrix_gpu = None; self.reputation_gpu = None\n",
        "        else: self.use_gpu = False\n",
        "\n",
        "    # ... (update_influence, batch_update_influences, update_reputation methods as before) ...\n",
        "    def update_influence(self, influencer: str, influenced: str, change: float):\n",
        "        if influencer in self.id_to_index and influenced in self.id_to_index:\n",
        "            i, j = self.id_to_index[influencer], self.id_to_index[influenced];\n",
        "            if i == j: return False\n",
        "            self.influence_matrix = self.influence_matrix_csr.tolil(); self.influence_matrix[i, j] = np.clip(self.influence_matrix[i, j] + change, 0, 1); self._convert_to_csr()\n",
        "            self.edge_changes_since_last_community_check += 1;\n",
        "            if self.use_gpu: self._transfer_to_gpu()\n",
        "            return True\n",
        "        return False\n",
        "    def batch_update_influences(self, updates: List[Tuple[str, str, float]]):\n",
        "        if not updates: return; self.influence_matrix = self.influence_matrix_csr.tolil(); num_actual_updates = 0\n",
        "        for influencer, influenced, change in updates:\n",
        "            if influencer in self.id_to_index and influenced in self.id_to_index:\n",
        "                i, j = self.id_to_index[influencer], self.id_to_index[influenced];\n",
        "                if i != j: self.influence_matrix[i, j] = np.clip(self.influence_matrix[i, j] + change, 0, 1); num_actual_updates += 1\n",
        "        self._convert_to_csr();\n",
        "        if self.use_gpu: self._transfer_to_gpu()\n",
        "        self.edge_changes_since_last_community_check += num_actual_updates\n",
        "    def update_reputation(self, agent_id: str, change: float):\n",
        "        if agent_id in self.id_to_index:\n",
        "            idx = self.id_to_index[agent_id]; self.reputation[idx] = np.clip(self.reputation[idx] + change, 0.1, 10.0)\n",
        "            if self.use_gpu and self.reputation_gpu is not None: self.reputation_gpu[idx] = cp.float32(self.reputation[idx])\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def get_peer_influence_for_agent(self, agent_id: str, normalize: bool = True) -> Dict[str, float]:\n",
        "        # ... (logic as before) ...\n",
        "        if agent_id in self.id_to_index:\n",
        "            target_idx = self.id_to_index[agent_id]; influence_on_agent = self.influence_matrix_csr[:, target_idx].toarray().flatten(); influences = {}; total_influence = 0\n",
        "            for i in range(self.num_agents):\n",
        "                if i != target_idx and influence_on_agent[i] > 1e-6: influencer_id = self.agent_ids[i]; weight = float(influence_on_agent[i]); influences[influencer_id] = weight; total_influence += weight\n",
        "            if normalize and total_influence > 1e-6: influences = {aid: w / total_influence for aid, w in influences.items()}\n",
        "            return influences\n",
        "        return {}\n",
        "\n",
        "    def get_influence_exerted_by_agent(self, agent_id: str) -> Dict[str, float]:\n",
        "        # ... (logic as before) ...\n",
        "         if agent_id in self.id_to_index:\n",
        "             influencer_idx = self.id_to_index[agent_id]; influence_by_agent = self.influence_matrix_csr[influencer_idx].toarray().flatten()\n",
        "             return {self.agent_ids[j]: float(influence_by_agent[j]) for j in range(self.num_agents) if j != influencer_idx and influence_by_agent[j] > 1e-6}\n",
        "         return {}\n",
        "\n",
        "    def get_top_influencers(self, n: int = 5, based_on: str = 'outgoing') -> List[Tuple[str, float]]:\n",
        "        # ... (logic as before) ...\n",
        "        n = min(n, self.num_agents); scores = None; xp = cp if self.use_gpu and self.influence_matrix_gpu is not None else np; matrix = self.influence_matrix_gpu if self.use_gpu and self.influence_matrix_gpu is not None else self.influence_matrix_csr\n",
        "        if based_on == 'reputation': scores = self.reputation_gpu if self.use_gpu and self.reputation_gpu is not None else self.reputation\n",
        "        elif based_on == 'incoming': scores = matrix.sum(axis=0); scores = scores.flatten() if isinstance(scores, xp.ndarray) else scores.A1 if hasattr(scores, \"A1\") else scores\n",
        "        else: scores = matrix.sum(axis=1); scores = scores.flatten() if isinstance(scores, xp.ndarray) else scores.A1 if hasattr(scores, \"A1\") else scores\n",
        "        if scores is None: return []\n",
        "        scores_cpu = cp.asnumpy(scores) if self.use_gpu and isinstance(scores, cp.ndarray) else np.asarray(scores).flatten()\n",
        "        if n < self.num_agents // 2: top_indices_unsorted = np.argpartition(scores_cpu, -n)[-n:]; top_indices = top_indices_unsorted[np.argsort(scores_cpu[top_indices_unsorted])][::-1]\n",
        "        else: top_indices = np.argsort(scores_cpu)[-n:][::-1]\n",
        "        return [(self.agent_ids[i], float(scores_cpu[i])) for i in top_indices]\n",
        "\n",
        "    def detect_communities(self, force_update: bool = False, threshold: float = 0.3) -> Optional[List[Set[str]]]: # Return Set[str]\n",
        "        recalculation_threshold = max(10, self.num_agents // 5)\n",
        "        if not force_update and self.last_communities is not None and self.edge_changes_since_last_community_check < recalculation_threshold: return self.last_communities\n",
        "        G = nx.Graph(); G.add_nodes_from(self.agent_ids); rows, cols = self.influence_matrix_csr.nonzero()\n",
        "        for i, j in zip(rows, cols):\n",
        "             weight = self.influence_matrix_csr[i, j];\n",
        "             if weight >= threshold: G.add_edge(self.agent_ids[i], self.agent_ids[j], weight=float(weight))\n",
        "        communities = None\n",
        "        if HAS_LOUVAIN:\n",
        "            try:\n",
        "                partition = community_louvain.best_partition(G, weight='weight'); community_map = defaultdict(set); [community_map[comm_id].add(node) for node, comm_id in partition.items()]; communities = list(community_map.values())\n",
        "            except Exception as e: warnings.warn(f\"Louvain failed: {e}. Fallback.\", RuntimeWarning); # Fall through to greedy\n",
        "        if communities is None: # Fallback if Louvain failed or not available\n",
        "            try: communities = [set(c) for c in nx.community.greedy_modularity_communities(G)]\n",
        "            except Exception as e: warnings.warn(f\"Community detection failed: {e}\", RuntimeWarning)\n",
        "        self.last_communities = communities; self.edge_changes_since_last_community_check = 0;\n",
        "        if self.message_bus and communities is not None: self.message_bus.publish('communities_detected', {'num': len(communities), 'sizes': [len(c) for c in communities]})\n",
        "        return self.last_communities\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\" Returns serializable state dictionary. \"\"\"\n",
        "        coo = self.influence_matrix_csr.tocoo()\n",
        "        matrix_state = list(zip(coo.row.tolist(), coo.col.tolist(), coo.data.tolist()))\n",
        "        return { \"version\": FRAMEWORK_VERSION, \"influence_matrix_coo\": matrix_state, \"reputation\": self.reputation.tolist(),\n",
        "                 \"last_communities\": [list(c) for c in self.last_communities] if self.last_communities else None, # Save as list of lists\n",
        "                 \"edge_changes\": self.edge_changes_since_last_community_check }\n",
        "\n",
        "    def load_state(self, state: Dict):\n",
        "        \"\"\" Loads state from dictionary. \"\"\"\n",
        "        loaded_version = state.get(\"version\")\n",
        "        if loaded_version != FRAMEWORK_VERSION: warnings.warn(f\"Loading social graph v '{loaded_version}' into v '{FRAMEWORK_VERSION}'.\", UserWarning)\n",
        "        self.reputation = np.array(state.get(\"reputation\", np.ones(self.num_agents)), dtype=np.float32)\n",
        "        matrix_state = state.get(\"influence_matrix_coo\")\n",
        "        if matrix_state:\n",
        "            try:\n",
        "                rows, cols, data = zip(*matrix_state)\n",
        "                self.influence_matrix_csr = csr_matrix((data, (rows, cols)), shape=(self.num_agents, self.num_agents), dtype=np.float32)\n",
        "                self.influence_matrix = self.influence_matrix_csr.tolil()\n",
        "            except ValueError: # Handle case where matrix_state might be empty\n",
        "                 warnings.warn(\"Could not reconstruct influence matrix from state (empty or invalid COO data?). Reinitializing.\", RuntimeWarning)\n",
        "                 self.influence_matrix = lil_matrix((self.num_agents, self.num_agents), dtype=np.float32); self._convert_to_csr()\n",
        "        else: self.influence_matrix = lil_matrix((self.num_agents, self.num_agents), dtype=np.float32); self._convert_to_csr()\n",
        "        loaded_communities = state.get(\"last_communities\")\n",
        "        self.last_communities = [set(c) for c in loaded_communities] if loaded_communities else None # Convert back to set\n",
        "        self.edge_changes_since_last_community_check = state.get(\"edge_changes\", 0)\n",
        "        if self.use_gpu: self._transfer_to_gpu() # Refresh GPU state\n",
        "\n"
      ],
      "metadata": {
        "id": "4uxuUUjLxugn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/components/resonance.py\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Any, TYPE_CHECKING\n",
        "from datetime import timedelta\n",
        "\n",
        "# Import framework version\n",
        "try:\n",
        "    from .. import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "# Forward reference for type hints\n",
        "if TYPE_CHECKING:\n",
        "    from .voice import VoiceEngine\n",
        "\n",
        "\n",
        "class TemporalResonanceTracker:\n",
        "    \"\"\" Tracks vectors over time for an agent. Includes serialization. \"\"\"\n",
        "    def __init__(self, agent_id: str, resonance_threshold: float = 0.82):\n",
        "        self.agent_id = agent_id\n",
        "        self.history: Dict[float, Dict[str, Any]] = {} # {timestamp: {\"vector\": list, \"theme\": str, \"content\": Dict}}\n",
        "        self.resonance_threshold = resonance_threshold\n",
        "\n",
        "    def record_state(self, timestamp: float, vector: np.ndarray, theme: str, content: Optional[Dict] = None):\n",
        "         if vector is not None and theme is not None:\n",
        "             self.history[timestamp] = {\"vector\": vector.tolist(), \"theme\": theme, \"content\": content or {}}\n",
        "\n",
        "    def detect_echo(self, current_vector: np.ndarray, current_theme: str) -> List[Dict]:\n",
        "        # ... (logic from previous combined file) ...\n",
        "        echoes = []\n",
        "        if current_vector is None or current_theme is None: return echoes\n",
        "        current_norm = np.linalg.norm(current_vector)\n",
        "        if current_norm < 1e-6: return echoes\n",
        "        past_data = [(ts, state.get('vector'), state.get('theme'), state.get('content')) for ts, state in self.history.items() if state.get('theme') == current_theme and state.get('vector')]\n",
        "        if not past_data: return echoes\n",
        "        try:\n",
        "            past_timestamps, past_vectors_list, past_themes, past_contents = zip(*past_data)\n",
        "            past_vectors_array = np.array(past_vectors_list); past_norms = np.linalg.norm(past_vectors_array, axis=1)\n",
        "            valid_indices = past_norms > 1e-6\n",
        "            if not np.any(valid_indices): return echoes\n",
        "            past_vectors_array = past_vectors_array[valid_indices]; past_norms = past_norms[valid_indices]; past_timestamps = np.array(past_timestamps)[valid_indices]\n",
        "            past_contents = [past_contents[i] for i, valid in enumerate(valid_indices) if valid]; past_themes = [past_themes[i] for i, valid in enumerate(valid_indices) if valid]\n",
        "            similarities = np.dot(past_vectors_array, current_vector) / (past_norms * current_norm); similarities = np.clip(similarities, 0.0, 1.0)\n",
        "            echo_indices = np.where(similarities > self.resonance_threshold)[0]\n",
        "            current_time = time.time()\n",
        "            for idx in echo_indices:\n",
        "                 ts = past_timestamps[idx]; echoes.append({\"similarity\": float(similarities[idx]), \"delta_t\": current_time - ts, \"previous_manifestation_timestamp\": float(ts), \"previous_manifestation_theme\": past_themes[idx], \"previous_manifestation_content\": past_contents[idx]})\n",
        "            return sorted(echoes, key=lambda x: x[\"similarity\"], reverse=True)\n",
        "        except ValueError as e: # Handle potential errors during unpacking or array creation\n",
        "            warnings.warn(f\"Agent {self.agent_id}: Error during echo detection - {e}\", RuntimeWarning)\n",
        "            return []\n",
        "\n",
        "\n",
        "    def get_history_summary(self, limit: int = 20) -> List[Dict]:\n",
        "         sorted_ts = sorted(self.history.keys(), reverse=True); summary = []\n",
        "         for ts in sorted_ts[:limit]:\n",
        "             state = self.history[ts]; vector_list = state.get('vector')\n",
        "             summary.append({\"timestamp\": ts, \"theme\": state.get(\"theme\"), \"vector_norm\": float(np.linalg.norm(vector_list)) if vector_list else 0.0, \"content_keys\": list(state.get(\"content\", {}).keys())})\n",
        "         return summary\n",
        "\n",
        "    def to_dict(self, history_limit: Optional[int] = None) -> Dict:\n",
        "        history_to_save = self.history\n",
        "        if history_limit is not None:\n",
        "            sorted_ts = sorted(self.history.keys(), reverse=True)[:history_limit]; history_to_save = {ts: self.history[ts] for ts in sorted_ts}\n",
        "        return { \"version\": FRAMEWORK_VERSION, \"resonance_threshold\": self.resonance_threshold, \"history\": history_to_save }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], agent_id: str) -> 'TemporalResonanceTracker':\n",
        "        loaded_version = data.get(\"version\")\n",
        "        if loaded_version != FRAMEWORK_VERSION: warnings.warn(f\"Agent {agent_id}: Loading resonance v '{loaded_version}' into v '{FRAMEWORK_VERSION}'.\", UserWarning)\n",
        "        instance = cls( agent_id=agent_id, resonance_threshold=data.get('resonance_threshold', 0.82) )\n",
        "        instance.history = data.get('history', {}) # Vectors are already lists\n",
        "        return instance\n",
        "\n",
        "\n",
        "class ResonanceLiturgy:\n",
        "    \"\"\" Handles the agent's 'ritual' response to temporal resonance. (Stateless, no serialization needed) \"\"\"\n",
        "    def __init__(self, agent_id: str, satori_threshold: float = 0.9):\n",
        "        self.agent_id = agent_id\n",
        "        self.satori_threshold = satori_threshold\n",
        "        self.ritual_phrases = [\"I recognize this shadow of myself.\", \"An older voice speaks through me.\", \"The past breathes anew...\",\n",
        "                               \"This pattern feels familiar...\", \"A thread connects me...\"] # Shortened\n",
        "\n",
        "    def _format_timedelta(self, delta_seconds: float) -> str:\n",
        "        # ... (timedelta formatting logic) ...\n",
        "        try:\n",
        "            delta = timedelta(seconds=int(delta_seconds)); days, rem_secs = delta.days, delta.seconds\n",
        "            hours, rem_secs = divmod(rem_secs, 3600); minutes, seconds = divmod(rem_secs, 60)\n",
        "            parts = []\n",
        "            if days > 0: parts.append(f\"{days}d\")\n",
        "            if hours > 0: parts.append(f\"{hours}h\")\n",
        "            if minutes > 0: parts.append(f\"{minutes}m\")\n",
        "            if not parts and seconds >= 0: parts.append(f\"{seconds}s\")\n",
        "            if not parts: return \"instant\"\n",
        "            return \" \".join(parts)\n",
        "        except ValueError: return f\"{delta_seconds:.0f}s\"\n",
        "\n",
        "\n",
        "    def compose_commentary(self, echo: dict) -> str:\n",
        "        elapsed_str = self._format_timedelta(echo[\"delta_t\"])\n",
        "        return (f\"Resonance {echo['similarity']:.3f} echoes self from ~{elapsed_str} ago. {random.choice(self.ritual_phrases)}\")\n",
        "\n",
        "    def generate_response_ritual(self, voice_engine: 'VoiceEngine', current_theme: str, past_theme: str) -> str:\n",
        "        # Need to import VoiceEngine for type hint or use string\n",
        "        prompt = f\"\"\"Context: Echo ({random.choice(self.ritual_phrases)}) connects '{current_theme}' to past '{past_theme}'.\n",
        "Task: Brief response acknowledging connection, linking past to present.\n",
        "Style: Arch: {voice_engine.linguistic_signature['archetype']}, Struct: {voice_engine.linguistic_signature['sentence_structure']}, Vocab: {voice_engine.linguistic_signature['vocabulary_richness']:.1f}\"\"\"\n",
        "        return voice_engine.generate_response(prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "mBlKo3IMxym8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/components/reflexivity.py\n",
        "import warnings\n",
        "import time\n",
        "from typing import TYPE_CHECKING, Optional\n",
        "\n",
        "# Import framework version\n",
        "try:\n",
        "    from .. import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "# Use TYPE_CHECKING for agent hint to avoid circular import\n",
        "if TYPE_CHECKING:\n",
        "    from ..agent import EnhancedAgent\n",
        "\n",
        "\n",
        "class ReflexivityLayer:\n",
        "    \"\"\" Handles agent self-reflection and meta-cognition. (State managed via agent ref, no extra serialization needed here) \"\"\"\n",
        "    def __init__(self, agent: 'EnhancedAgent'):\n",
        "         \"\"\" Initializes with a reference to the owning agent. \"\"\"\n",
        "         if not hasattr(agent, 'agent_id'): # Basic check for valid agent object\n",
        "             raise TypeError(\"Agent reference is required for ReflexivityLayer.\")\n",
        "         self.agent = agent\n",
        "\n",
        "    def force_deep_reflection(self, trigger: str):\n",
        "        \"\"\" Initiate identity-realignment sequence (Satori Event). \"\"\"\n",
        "        # print(f\"Agent {self.agent.agent_id} entering deep reflection triggered by: {trigger}\") # Verbose\n",
        "        if not all(hasattr(self.agent, attr) for attr in ['voice', 'memory', 'cognitive']):\n",
        "             warnings.warn(f\"Agent {self.agent.agent_id}: Missing components for deep reflection.\", RuntimeWarning)\n",
        "             return\n",
        "\n",
        "        old_style = self.agent.voice.linguistic_signature.copy()\n",
        "        # Evolve voice style\n",
        "        self.agent.voice.evolve_style(influence={\"archetype\": \"enlightened\", \"sentence_structure\": \"paradoxical\"})\n",
        "        # Add Satori memory event\n",
        "        satori_memory_content = {\n",
        "            \"type\": \"satori_event\", \"trigger\": trigger, \"timestamp\": time.time(), \"theme\": \"self_reflection\",\n",
        "            \"reflection_details\": { \"old_style_archetype\": old_style.get(\"archetype\"),\n",
        "                                    \"new_style_archetype\": self.agent.voice.linguistic_signature.get(\"archetype\") }\n",
        "        }\n",
        "        # Use agent's memory component to add memory\n",
        "        self.agent.memory.add_memory(satori_memory_content, importance=1.0)\n",
        "\n",
        "        # Optional: Trigger cognitive heuristic changes here if desired\n",
        "        # e.g., self.agent.cognitive.heuristics = self._apply_satori_heuristic_shift(self.agent.cognitive.heuristics)\n",
        "\n",
        "        if self.agent.message_bus:\n",
        "             self.agent.message_bus.publish('agent_satori_event', {'agent_id': self.agent.agent_id, 'trigger': trigger})\n",
        "        # print(f\"Agent {self.agent.agent_id} completed deep reflection.\") # Verbose\n",
        "\n",
        "    # Optional helper for heuristic shifts during satori\n",
        "    # def _apply_satori_heuristic_shift(self, current_heuristics):\n",
        "    #     # Example: Increase novelty/creativity focus\n",
        "    #     shifted = current_heuristics.copy()\n",
        "    #     shifted[2, 1] += 0.1 # Novelty -> Creative\n",
        "    #     shifted[2, 2] += 0.1 # Novelty -> Balanced\n",
        "    #     return np.clip(shifted, 0.1, 0.9)\n",
        "\n"
      ],
      "metadata": {
        "id": "VBAkP8eZx2N2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/agent.py\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# Import framework version\n",
        "try:\n",
        "    from . import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "# Import components using relative paths\n",
        "from .components.memory import MemoryContinuumLayer\n",
        "from .components.cognitive import CognitiveDiversityEngine\n",
        "from .components.voice import VoiceEngine\n",
        "from .components.resonance import TemporalResonanceTracker, ResonanceLiturgy\n",
        "from .components.reflexivity import ReflexivityLayer\n",
        "from .utils.message_bus import MessageBus # Assuming message_bus is in utils\n",
        "\n",
        "class EnhancedAgent:\n",
        "    \"\"\" Represents a single agent, integrating components. Includes serialization. \"\"\"\n",
        "    def __init__(self, agent_id: str, personality: Dict, capacity: int = 100,\n",
        "                 message_bus: Optional[MessageBus] = None, use_semantic: bool = True,\n",
        "                 # Allow passing pre-constructed components for loading state\n",
        "                 memory: Optional[MemoryContinuumLayer] = None,\n",
        "                 cognitive: Optional[CognitiveDiversityEngine] = None,\n",
        "                 voice: Optional[VoiceEngine] = None,\n",
        "                 temporal_resonance: Optional[TemporalResonanceTracker] = None,\n",
        "                 add_initial_memory: bool = True): # Flag to control initial memory\n",
        "        self.agent_id = agent_id\n",
        "        self.message_bus = message_bus\n",
        "\n",
        "        # Initialize components, using provided ones if available (for loading)\n",
        "        self.memory = memory if memory is not None else \\\n",
        "                      MemoryContinuumLayer(agent_id, capacity, use_semantic, message_bus)\n",
        "        self.cognitive = cognitive if cognitive is not None else \\\n",
        "                         CognitiveDiversityEngine(agent_id, personality, self.memory, message_bus)\n",
        "        self.voice = voice if voice is not None else \\\n",
        "                     VoiceEngine(agent_id) # Pass initial style from config if needed\n",
        "        self.temporal_resonance = temporal_resonance if temporal_resonance is not None else \\\n",
        "                                  TemporalResonanceTracker(agent_id)\n",
        "        self.reflexivity_layer = ReflexivityLayer(self) # Always needs self reference\n",
        "        self.resonance_liturgy_instance = ResonanceLiturgy(agent_id) # Stateless, init normally\n",
        "\n",
        "        # Agent-level state\n",
        "        self.last_reflection_trigger: Optional[str] = None\n",
        "        self.recent_decision_log: List[Dict] = [] # Runtime log\n",
        "\n",
        "        # Add initial memory only if specified (i.e., not loading from state)\n",
        "        if add_initial_memory:\n",
        "            self.memory.add_memory({\"type\": \"initial_state\", \"theme\": \"genesis\", \"timestamp\": time.time()}, importance=0.7)\n",
        "\n",
        "    # ... (simulation_step, check_resonance methods as before) ...\n",
        "    def simulation_step(self, situational_entropy: float, peer_influence: Dict[str, float], query: Optional[str] = None):\n",
        "         self.cognitive.update_heuristics(situational_entropy)\n",
        "         decision_response = None\n",
        "         if query:\n",
        "             decision_response = self.cognitive.decide(query, peer_influence)\n",
        "         current_theme = self.memory.get_current_focus_theme()\n",
        "         current_style_vector = self.voice.linguistic_signature.get(\"style_vector\")\n",
        "\n",
        "         if current_style_vector is not None:\n",
        "             current_content = {\"cognitive_state\": self.cognitive.cognitive_state.tolist()}\n",
        "             self.temporal_resonance.record_state(time.time(), current_style_vector, current_theme, current_content)\n",
        "\n",
        "         self.check_resonance()\n",
        "         self.memory.update_all_memories()\n",
        "\n",
        "         return decision_response\n",
        "\n",
        "    def check_resonance(self):\n",
        "        current_style_vector = self.voice.linguistic_signature.get(\"style_vector\")\n",
        "        current_theme = self.memory.get_current_focus_theme()\n",
        "        if current_style_vector is None:\n",
        "            return\n",
        "\n",
        "        echoes = self.temporal_resonance.detect_echo(current_style_vector, current_theme)\n",
        "        if not echoes:\n",
        "            return\n",
        "\n",
        "        liturgy = self.resonance_liturgy_instance\n",
        "        top_echo = echoes[0]\n",
        "        commentary = liturgy.compose_commentary(top_echo)\n",
        "\n",
        "        resonance_memory_content = {\n",
        "            \"type\": \"resonance_event\",\n",
        "            \"theme\": current_theme,\n",
        "            \"echo_strength\": top_echo[\"similarity\"],\n",
        "            \"delta_t\": top_echo[\"delta_t\"],\n",
        "            \"previous_manifestation_timestamp\": top_echo[\"previous_manifestation_timestamp\"],\n",
        "            \"previous_manifestation_theme\": top_echo[\"previous_manifestation_theme\"],\n",
        "            \"reflection\": commentary,\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "        resonance_memory_id = self.memory.add_memory(resonance_memory_content, importance=np.clip(0.85 * top_echo[\"similarity\"], 0.1, 1.0))\n",
        "\n",
        "        meaningful_connection_threshold = 0.75\n",
        "        if top_echo[\"similarity\"] > meaningful_connection_threshold:\n",
        "            past_theme = top_echo[\"previous_manifestation_theme\"]\n",
        "            response_text = liturgy.generate_response_ritual(self.voice, current_theme, past_theme)\n",
        "            response_memory_content = {\n",
        "                \"type\": \"resonant_reply\",\n",
        "                \"theme\": current_theme,\n",
        "                \"message\": response_text,\n",
        "                \"responding_to_echo_at\": top_echo[\"previous_manifestation_timestamp\"],\n",
        "                \"linked_resonance_event_id\": resonance_memory_id,\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "            reply_memory_id = self.memory.add_memory(response_memory_content, importance=0.75)\n",
        "            self.memory.link_memories(resonance_memory_id, reply_memory_id, \"generated_reply\")\n",
        "\n",
        "        if top_echo[\"similarity\"] >= liturgy.satori_threshold:\n",
        "            trigger_message = (\n",
        "                f\"Strong echo ({top_echo['similarity']:.3f}) detected connecting theme '{current_theme}' \"\n",
        "                f\"to past self (theme: '{top_echo['previous_manifestation_theme']}').\"\n",
        "            )\n",
        "            self.last_reflection_trigger = trigger_message\n",
        "            self.reflexivity_layer.force_deep_reflection(trigger=trigger_message)\n",
        "\n",
        "        if self.message_bus:\n",
        "            self.message_bus.publish('agent_resonance_detected', {\n",
        "                'agent_id': self.agent_id,\n",
        "                'echo_strength': top_echo['similarity'],\n",
        "                'delta_t': top_echo['delta_t'],\n",
        "                'current_theme': current_theme,\n",
        "                'past_theme': top_echo['previous_manifestation_theme'],\n",
        "                'satori_triggered': top_echo['similarity'] >= liturgy.satori_threshold\n",
        "            })\n",
        "\n",
        "\n",
        "    def to_dict(self, include_memory_embeddings: bool = False, resonance_history_limit: Optional[int] = 50) -> Dict[str, Any]:\n",
        "        \"\"\" Serializes the agent's state using component to_dict methods. \"\"\"\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"version\": FRAMEWORK_VERSION,\n",
        "            \"cognitive_state\": self.cognitive.to_dict(history_limit=5),\n",
        "            \"voice_state\": self.voice.to_dict(),\n",
        "            \"temporal_resonance_state\": self.temporal_resonance.to_dict(history_limit=resonance_history_limit),\n",
        "            \"memory_state\": self.memory.to_dict(include_embeddings=include_memory_embeddings),\n",
        "            \"last_reflection_trigger\": self.last_reflection_trigger,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], message_bus: Optional[MessageBus] = None, strict_validation: bool = True) -> 'EnhancedAgent':\n",
        "        \"\"\" Reconstructs an EnhancedAgent using component from_dict methods. \"\"\"\n",
        "        agent_id = data['agent_id']\n",
        "        loaded_version = data.get(\"version\")\n",
        "        if loaded_version != FRAMEWORK_VERSION:\n",
        "            warnings.warn(f\"Agent {agent_id}: Loading state v '{loaded_version}' vs current '{FRAMEWORK_VERSION}'.\", UserWarning)\n",
        "\n",
        "        # Reconstruct components first\n",
        "        try:\n",
        "            memory = MemoryContinuumLayer.from_dict(data['memory_state'], message_bus=message_bus)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Agent {agent_id}: Failed memory load: {e}\", RuntimeWarning)\n",
        "            raise ValueError(\"Memory load failed\") from e\n",
        "        try:\n",
        "            cognitive = CognitiveDiversityEngine.from_dict(data['cognitive_state'], agent_id=agent_id, memory_layer=memory, message_bus=message_bus)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Agent {agent_id}: Failed cognitive load: {e}\", RuntimeWarning)\n",
        "            raise ValueError(\"Cognitive load failed\") from e\n",
        "        try:\n",
        "            voice = VoiceEngine.from_dict(data['voice_state'], agent_id=agent_id)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Agent {agent_id}: Failed voice load: {e}\", RuntimeWarning)\n",
        "            raise ValueError(\"Voice load failed\") from e\n",
        "        try:\n",
        "            temporal_resonance = TemporalResonanceTracker.from_dict(data['temporal_resonance_state'], agent_id=agent_id)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Agent {agent_id}: Failed resonance load: {e}\", RuntimeWarning)\n",
        "            raise ValueError(\"Resonance load failed\") from e\n",
        "\n",
        "        # Create agent instance, passing reconstructed components, and DO NOT add initial memory\n",
        "        agent = cls(\n",
        "            agent_id=agent_id,\n",
        "            personality=cognitive.personality, # Get personality from loaded cognitive state\n",
        "            capacity=memory.capacity,\n",
        "            message_bus=message_bus,\n",
        "            use_semantic=memory.use_semantic,\n",
        "            memory=memory,\n",
        "            cognitive=cognitive,\n",
        "            voice=voice,\n",
        "            temporal_resonance=temporal_resonance,\n",
        "            add_initial_memory=False\n",
        "        ) # Important flag\n",
        "\n",
        "        # Load remaining agent-level state\n",
        "        agent.last_reflection_trigger = data.get(\"last_reflection_trigger\")\n",
        "        # agent.recent_decision_log = [] # Don't load runtime log\n",
        "\n",
        "        # --- Validation ---\n",
        "        try:\n",
        "            agent._validate_state(strict=strict_validation)\n",
        "        except ValueError as e:\n",
        "            if strict_validation:\n",
        "                raise e\n",
        "            else:\n",
        "                warnings.warn(f\"Agent {agent_id}: State validation failed post-load: {e}\", RuntimeWarning)\n",
        "        # print(f\"Agent {agent_id} reconstructed.\") # Verbose\n",
        "        return agent\n",
        "\n",
        "    def _validate_state(self, strict: bool = True):\n",
        "        # ... (validation logic as before) ...\n",
        "        errors = []\n",
        "        warnings_list = []\n",
        "        if not isinstance(self.cognitive.personality, dict):\n",
        "            errors.append(\"Personality type.\")\n",
        "        else:\n",
        "            for key, val in self.cognitive.personality.items():\n",
        "                 if not (0.0 <= val <= 1.0):\n",
        "                     errors.append(f\"Personality '{key}' range.\")\n",
        "        if not isinstance(self.cognitive.heuristics, np.ndarray) or self.cognitive.heuristics.shape != (4, 4):\n",
        "            errors.append(\"Heuristics shape.\")\n",
        "        if not isinstance(self.cognitive.cognitive_state, np.ndarray) or self.cognitive.cognitive_state.shape != (4,):\n",
        "            errors.append(\"Cognitive state shape.\")\n",
        "        elif not np.isclose(np.sum(self.cognitive.cognitive_state), 1.0):\n",
        "            warnings_list.append(f\"Cognitive state sum ~{np.sum(self.cognitive.cognitive_state):.2f}.\")\n",
        "        sig = self.voice.linguistic_signature\n",
        "        if not isinstance(sig, dict):\n",
        "            errors.append(\"Ling sig type.\")\n",
        "        elif \"style_vector\" not in sig or not isinstance(sig[\"style_vector\"], np.ndarray):\n",
        "            errors.append(\"Style vector type/missing.\")\n",
        "        elif np.any(np.isnan(sig[\"style_vector\"])) or np.any(np.isinf(sig[\"style_vector\"])):\n",
        "            errors.append(\"Style vector NaN/Inf.\")\n",
        "        if not isinstance(self.memory.memories, dict):\n",
        "            errors.append(\"Memory store type.\")\n",
        "        # Correct state sum if not strict\n",
        "        if not strict and any(\"Cognitive state sum\" in w for w in warnings_list):\n",
        "            if np.sum(self.cognitive.cognitive_state) > 1e-6:\n",
        "                self.cognitive.cognitive_state /= np.sum(self.cognitive.cognitive_state)\n",
        "            else:\n",
        "                self.cognitive.cognitive_state = np.ones(4) / 4\n",
        "                warnings_list.append(\"Cognitive state reset.\")\n",
        "        # Report\n",
        "        for w in warnings_list:\n",
        "            warnings.warn(f\"Agent {self.agent_id} Validate Warn: {w}\", RuntimeWarning)\n",
        "        if errors:\n",
        "            error_message = f\"Agent {self.agent_id} Validate Fail: {'; '.join(errors)}\";\n",
        "        if strict and errors:\n",
        "            raise ValueError(error_message)\n",
        "        elif errors:\n",
        "            warnings.warn(error_message, RuntimeWarning)"
      ],
      "metadata": {
        "id": "nZun-o-Fx5Yc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "d717fcb0-e208-4131-bc92-8873b0666d0d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'agisa_sac'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-2085372983.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Import components using absolute paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magisa_sac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemoryContinuumLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magisa_sac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcognitive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCognitiveDiversityEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magisa_sac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVoiceEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agisa_sac'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/analysis/__init__.py\n",
        "\"\"\"Modules for analyzing simulation state and results.\"\"\"\n",
        "\n",
        "from .analyzer import AgentStateAnalyzer\n",
        "from .exporter import ChronicleExporter\n",
        "from .tda import PersistentHomologyTracker\n",
        "from .visualization import plot_persistence_diagram, plot_persistence_barcode, plot_metric_comparison\n",
        "from .clustering import cluster_archetypes\n",
        "\n",
        "__all__ = [\n",
        "    \"AgentStateAnalyzer\",\n",
        "    \"ChronicleExporter\",\n",
        "    \"PersistentHomologyTracker\",\n",
        "    \"plot_persistence_diagram\",\n",
        "    \"plot_persistence_barcode\",\n",
        "    \"plot_metric_comparison\",\n",
        "    \"cluster_archetypes\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "kIW1S4cyx9AA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "0ed9276d-93d5-46cf-8f73-8a0fbcd5ef05"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "attempted relative import with no known parent package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-2587124426.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\"\"\"Modules for analyzing simulation state and results.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgentStateAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChronicleExporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPersistentHomologyTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/analysis/analyzer.py\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, List, Optional, Any, TYPE_CHECKING\n",
        "\n",
        "# Use TYPE_CHECKING for agent hint if EnhancedAgent imports this module\n",
        "if TYPE_CHECKING:\n",
        "    from ..agent import EnhancedAgent\n",
        "\n",
        "class AgentStateAnalyzer:\n",
        "    \"\"\" Computes system-wide metrics based on the current state of all agents. \"\"\"\n",
        "    def __init__(self, agents: Dict[str, 'EnhancedAgent']):\n",
        "        if not isinstance(agents, dict): raise TypeError(\"Input 'agents' must be a dictionary.\")\n",
        "        self.agents = agents\n",
        "        self.num_agents = len(agents)\n",
        "\n",
        "    def compute_archetype_distribution(self) -> Dict[str, int]:\n",
        "        \"\"\" Calculates the frequency distribution of declared agent archetypes. \"\"\"\n",
        "        if not self.agents: return {}\n",
        "        return Counter(agent.voice.linguistic_signature.get(\"archetype\", \"unknown\") for agent in self.agents.values() if hasattr(agent, 'voice'))\n",
        "\n",
        "    def compute_satori_wave_ratio(self, threshold: float = 0.88) -> float:\n",
        "        \"\"\" Calculates proportion of agents meeting satori echo threshold. (Canonical) \"\"\"\n",
        "        if not self.agents: return 0.0\n",
        "        satori_count = 0\n",
        "        for agent in self.agents.values():\n",
        "            if not all(hasattr(agent, attr) for attr in ['temporal_resonance', 'voice', 'memory']): continue\n",
        "            current_style_vector = agent.voice.linguistic_signature.get(\"style_vector\");\n",
        "            try: current_theme = agent.memory.get_current_focus_theme(); except Exception: current_theme = None\n",
        "            if current_style_vector is None or current_theme is None: continue\n",
        "            detected_echoes = agent.temporal_resonance.detect_echo(current_style_vector, current_theme)\n",
        "            if detected_echoes and detected_echoes[0][\"similarity\"] >= threshold: satori_count += 1\n",
        "        return satori_count / self.num_agents if self.num_agents > 0 else 0.0\n",
        "\n",
        "    def compute_archetype_entropy(self, distribution: Optional[Dict[str, int]] = None) -> float:\n",
        "        \"\"\" Calculates the Shannon entropy of the archetype distribution. \"\"\"\n",
        "        if distribution is None: distribution = self.compute_archetype_distribution()\n",
        "        if not distribution: return 0.0\n",
        "        total_agents = sum(distribution.values());\n",
        "        if total_agents == 0: return 0.0\n",
        "        entropy = 0.0\n",
        "        for count in distribution.values():\n",
        "            if count > 0: probability = count / total_agents; entropy -= probability * math.log2(probability)\n",
        "        return entropy\n",
        "\n",
        "    def compute_mean_resonance_strength(self) -> float:\n",
        "        \"\"\" Calculates the average similarity of the strongest echo for agents with echoes. \"\"\"\n",
        "        if not self.agents: return 0.0\n",
        "        similarities = []\n",
        "        for agent in self.agents.values():\n",
        "            if not all(hasattr(agent, attr) for attr in ['temporal_resonance', 'voice', 'memory']): continue\n",
        "            current_style_vector = agent.voice.linguistic_signature.get(\"style_vector\");\n",
        "            try: current_theme = agent.memory.get_current_focus_theme(); except Exception: current_theme = None\n",
        "            if current_style_vector is None or current_theme is None: continue\n",
        "            detected_echoes = agent.temporal_resonance.detect_echo(current_style_vector, current_theme)\n",
        "            if detected_echoes: similarities.append(detected_echoes[0][\"similarity\"])\n",
        "        return float(np.mean(similarities)) if similarities else 0.0\n",
        "\n",
        "    def summarize(self, satori_threshold: float = 0.88) -> Dict[str, Any]:\n",
        "        \"\"\" Computes and returns a dictionary containing all key system metrics. \"\"\"\n",
        "        if not self.agents: return {\"satori_wave_ratio\": 0.0, \"archetype_distribution\": {}, \"archetype_entropy\": 0.0, \"mean_resonance_strength\": 0.0, \"agent_count\": 0}\n",
        "        distribution = self.compute_archetype_distribution()\n",
        "        summary = {\"satori_wave_ratio\": self.compute_satori_wave_ratio(threshold=satori_threshold), \"archetype_distribution\": distribution,\n",
        "                   \"archetype_entropy\": self.compute_archetype_entropy(distribution=distribution), \"mean_resonance_strength\": self.compute_mean_resonance_strength(),\n",
        "                   \"agent_count\": self.num_agents}\n",
        "        return summary\n"
      ],
      "metadata": {
        "id": "gVpZ2cKdyBgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "15ee3493-55d8-484e-c6d5-279e9611f599"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-33-2910673108.py, line 30)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-33-2910673108.py\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    try: current_theme = agent.memory.get_current_focus_theme(); except Exception: current_theme = None\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/analysis/exporter.py\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, TYPE_CHECKING\n",
        "\n",
        "# Use TYPE_CHECKING for chronicler hint\n",
        "if TYPE_CHECKING:\n",
        "    from ..chronicler import ResonanceChronicler # Adjust if chronicler is moved\n",
        "\n",
        "class ChronicleExporter:\n",
        "    \"\"\" Handles generation and export of formatted narrative outputs. \"\"\"\n",
        "    def __init__(self, chronicler: 'ResonanceChronicler'):\n",
        "        if chronicler is None: raise ValueError(\"Chronicler instance required.\")\n",
        "        self.chronicler = chronicler\n",
        "\n",
        "    def format_lineage_scroll_markdown(self, agent_id: str, include_cognitive_state: bool = True) -> Optional[str]:\n",
        "        \"\"\" Formats the lineage of a specific agent into a Markdown string. \"\"\"\n",
        "        lineage = self.chronicler.lineages.get(agent_id, [])\n",
        "        if not lineage: return None\n",
        "        report = [f\"# Resonance Lineage Scroll: {agent_id}\\n\", f\"*(Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')})*\\n\"]\n",
        "        for i, epoch_entry in enumerate(lineage):\n",
        "            report.append(f\"## Agent Epoch {i+1}: Theme '{epoch_entry.theme}'\")\n",
        "            try: ts_str = datetime.fromtimestamp(epoch_entry.timestamp).strftime('%Y-%m-%d %H:%M:%S'); except Exception: ts_str = f\"TS {epoch_entry.timestamp}\"\n",
        "            report.append(f\"- **Timestamp**: {ts_str}\")\n",
        "            if include_cognitive_state and epoch_entry.cognitive_state is not None: report.append(f\"- **Cognitive State (R,R,N,S)**: [{', '.join([f'{s:.3f}' for s in epoch_entry.cognitive_state])}]\")\n",
        "            if epoch_entry.echo_strength is not None: report.append(f\"- **Resonance Echo Strength**: {epoch_entry.echo_strength:.4f}\")\n",
        "            if epoch_entry.reflection: report.append(f\"\\n> {epoch_entry.reflection}\\n\")\n",
        "            report.append(\"---\")\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "    def generate_echo_manifesto(self, agent_id: str, min_echo_strength: float = 0.85) -> Optional[str]:\n",
        "        \"\"\" Generates a focused report highlighting significant resonance events. \"\"\"\n",
        "        lineage = self.chronicler.lineages.get(agent_id, []);\n",
        "        if not lineage: return None\n",
        "        manifesto_entries = []\n",
        "        for i, entry in enumerate(lineage):\n",
        "            if entry.echo_strength is not None and entry.echo_strength >= min_echo_strength:\n",
        "                try: ts_str = datetime.fromtimestamp(entry.timestamp).strftime('%Y-%m-%d %H:%M'); except Exception: ts_str = f\"TS {entry.timestamp:.0f}\"\n",
        "                manifesto_entries.append({\"epoch\": i + 1, \"timestamp_str\": ts_str, \"theme\": entry.theme, \"strength\": entry.echo_strength, \"reflection\": entry.reflection or \"*No reflection*\"})\n",
        "        if not manifesto_entries: return None\n",
        "        output = [f\"# Echo Manifesto: {agent_id}\\n\", f\"*(Significant Resonance >= {min_echo_strength:.2f})*\\n\"]\n",
        "        for entry in sorted(manifesto_entries, key=lambda x: x['strength'], reverse=True):\n",
        "             output.append(f\"## Agent Epoch {entry['epoch']} ({entry['timestamp_str']}) - Strength: {entry['strength']:.4f}\")\n",
        "             output.append(f\"**Theme:** {entry['theme']}\"); output.append(f\"> {entry['reflection']}\"); output.append(\"---\")\n",
        "        return \"\\n\".join(output)\n",
        "\n",
        "    def export_lineage_scroll(self, agent_id: str, directory: str = \"./scrolls\", filename: Optional[str] = None) -> Optional[str]:\n",
        "        \"\"\" Generates and saves the lineage scroll Markdown file. \"\"\"\n",
        "        scroll_content = self.format_lineage_scroll_markdown(agent_id);\n",
        "        if scroll_content is None: print(f\"No lineage for {agent_id}.\"); return None\n",
        "        if filename is None: filename = f\"{agent_id}_lineage_scroll.md\"; filepath = os.path.join(directory, filename)\n",
        "        try: os.makedirs(directory, exist_ok=True);\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f: f.write(scroll_content); print(f\"Scroll saved: {filepath}\"); return filepath\n",
        "        except IOError as e: warnings.warn(f\"Failed save scroll {agent_id}: {e}\", RuntimeWarning); return None\n",
        "\n",
        "    def export_echo_manifesto(self, agent_id: str, directory: str = \"./manifestos\", filename: Optional[str] = None, min_echo_strength: float = 0.85) -> Optional[str]:\n",
        "        \"\"\" Generates and saves the echo manifesto Markdown file. \"\"\"\n",
        "        manifesto_content = self.generate_echo_manifesto(agent_id, min_echo_strength);\n",
        "        if manifesto_content is None: print(f\"No echoes >= {min_echo_strength:.2f} for {agent_id}.\"); return None\n",
        "        if filename is None: filename = f\"{agent_id}_echo_manifesto.md\"; filepath = os.path.join(directory, filename)\n",
        "        try: os.makedirs(directory, exist_ok=True);\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as f: f.write(manifesto_content); print(f\"Manifesto saved: {filepath}\"); return filepath\n",
        "        except IOError as e: warnings.warn(f\"Failed save manifesto {agent_id}: {e}\", RuntimeWarning); return None\n",
        "\n",
        "    def export_all_scrolls(self, directory: str = \"./scrolls\"):\n",
        "        \"\"\" Exports lineage scrolls for all agents. \"\"\"\n",
        "        count = 0; agent_ids = list(self.chronicler.lineages.keys()); print(f\"Exporting {len(agent_ids)} scrolls to {directory}...\")\n",
        "        for agent_id in agent_ids:\n",
        "            if self.export_lineage_scroll(agent_id, directory): count += 1\n",
        "        print(f\"Exported {count} scrolls.\")\n",
        "\n",
        "    def export_all_manifestos(self, directory: str = \"./manifestos\", min_echo_strength: float = 0.85):\n",
        "        \"\"\" Exports echo manifestos for all agents with significant echoes. \"\"\"\n",
        "        count = 0; agent_ids = list(self.chronicler.lineages.keys()); print(f\"Exporting manifestos (>{min_echo_strength:.2f}) for {len(agent_ids)} agents to {directory}...\")\n",
        "        for agent_id in agent_ids:\n",
        "             if self.export_echo_manifesto(agent_id, directory, min_echo_strength=min_echo_strength): count += 1\n",
        "        print(f\"Exported {count} manifestos.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WCZq4Sj0yDuM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "3c378f71-15d2-4e87-d9f3-e454d29195eb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-34-2316974106.py, line 25)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-34-2316974106.py\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    try: ts_str = datetime.fromtimestamp(epoch_entry.timestamp).strftime('%Y-%m-%d %H:%M:%S'); except Exception: ts_str = f\"TS {epoch_entry.timestamp}\"\u001b[0m\n\u001b[0m                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/analysis/tda.py\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# Import framework version\n",
        "try:\n",
        "    from .. import FRAMEWORK_VERSION\n",
        "except ImportError:\n",
        "    FRAMEWORK_VERSION = \"unknown\"\n",
        "\n",
        "# TDA Dependency Handling\n",
        "try: import ripser; HAS_RIPSER = True\n",
        "except ImportError: HAS_RIPSER = False; warnings.warn(\"`ripser` N/A. TDA compute disabled.\", ImportWarning)\n",
        "try: import persim; HAS_PERSIM = True\n",
        "except ImportError: HAS_PERSIM = False; warnings.warn(\"`persim` N/A. TDA distance disabled.\", ImportWarning)\n",
        "\n",
        "\n",
        "class PersistentHomologyTracker:\n",
        "    \"\"\" Performs TDA using persistent homology. Includes serialization. \"\"\"\n",
        "    def __init__(self, max_dimension: int = 1):\n",
        "        self.max_dimension = max_dimension\n",
        "        self.persistence_diagrams_history: List[Optional[List[np.ndarray]]] = []\n",
        "        self.has_tda_lib = HAS_RIPSER # Store availability\n",
        "\n",
        "    def compute_persistence(self, point_cloud: np.ndarray, max_radius: Optional[float] = None, **ripser_kwargs) -> Optional[List[np.ndarray]]:\n",
        "        \"\"\" Computes persistence diagram using ripser. \"\"\"\n",
        "        if not self.has_tda_lib or point_cloud is None or point_cloud.ndim != 2 or point_cloud.shape[0] < 2:\n",
        "            self.persistence_diagrams_history.append(None); return None\n",
        "        try:\n",
        "            default_kwargs = {'maxdim': self.max_dimension, 'thresh': max_radius if max_radius is not None else np.inf}; default_kwargs.update(ripser_kwargs)\n",
        "            result = ripser.ripser(point_cloud, **default_kwargs); diagrams = result['dgms']\n",
        "            cleaned_diagrams = []\n",
        "            for dim, diag in enumerate(diagrams):\n",
        "                 if diag.shape[0] > 0:\n",
        "                     if dim == 0:\n",
        "                         finite_bars = diag[diag[:, 1] != np.inf]; inf_bars = diag[diag[:, 1] == np.inf]\n",
        "                         if inf_bars.shape[0] > 0: inf_bars = inf_bars[np.argsort(inf_bars[:, 0])[:1]]; cleaned_diag = np.vstack((finite_bars, inf_bars)) if finite_bars.shape[0] > 0 else inf_bars\n",
        "                         else: cleaned_diag = finite_bars\n",
        "                     else: cleaned_diag = diag[diag[:, 1] != np.inf]\n",
        "                     cleaned_diagrams.append(cleaned_diag)\n",
        "                 else: cleaned_diagrams.append(np.empty((0, 2)))\n",
        "            self.persistence_diagrams_history.append(cleaned_diagrams); return cleaned_diagrams\n",
        "        except Exception as e: warnings.warn(f\"Persistence computation failed: {e}\", RuntimeWarning); self.persistence_diagrams_history.append(None); return None\n",
        "\n",
        "    def detect_phase_transition(self, comparison_dimension: int = 1, distance_metric: str = 'bottleneck', threshold: float = 0.2) -> Tuple[bool, float]:\n",
        "        \"\"\" Detects phase transitions by comparing diagrams using persim. Returns (detected, distance). \"\"\"\n",
        "        if not HAS_PERSIM or len(self.persistence_diagrams_history) < 2: return False, 0.0\n",
        "        current_diagram_list = self.persistence_diagrams_history[-1]; previous_diagram_list = self.persistence_diagrams_history[-2]\n",
        "        if current_diagram_list is None or previous_diagram_list is None or len(current_diagram_list) <= comparison_dimension or len(previous_diagram_list) <= comparison_dimension: return False, 0.0\n",
        "        current_diagram = np.array(current_diagram_list[comparison_dimension]); previous_diagram = np.array(previous_diagram_list[comparison_dimension])\n",
        "        distance = 0.0\n",
        "        if current_diagram.shape[0] == 0 and previous_diagram.shape[0] == 0: distance = 0.0\n",
        "        elif current_diagram.shape[0] == 0 or previous_diagram.shape[0] == 0: distance = threshold + 0.1 # Assume change if features appear/vanish\n",
        "        else:\n",
        "             try:\n",
        "                 if distance_metric == 'bottleneck': distance, _ = persim.bottleneck(current_diagram, previous_diagram, matching=False)\n",
        "                 elif distance_metric == 'wasserstein': distance, _ = persim.wasserstein(current_diagram, previous_diagram, matching=False, p=2)\n",
        "                 else: warnings.warn(f\"Unsupported TDA metric: {distance_metric}. Using Bottleneck.\", RuntimeWarning); distance, _ = persim.bottleneck(current_diagram, previous_diagram, matching=False)\n",
        "             except Exception as e: warnings.warn(f\"TDA distance failed ({distance_metric}, dim={comparison_dimension}): {e}\", RuntimeWarning); return False, 0.0\n",
        "        transition_detected = distance > threshold\n",
        "        return transition_detected, float(distance) # Return distance as well\n",
        "\n",
        "    def get_diagram_summary(self, diagram_index: int = -1) -> Dict:\n",
        "        \"\"\" Returns summary stats for a specific diagram in history. \"\"\"\n",
        "        if not self.persistence_diagrams_history or diagram_index >= len(self.persistence_diagrams_history) or self.persistence_diagrams_history[diagram_index] is None:\n",
        "             return {\"error\": \"Diagram not available\"}\n",
        "        summary = {}\n",
        "        diagram_list = self.persistence_diagrams_history[diagram_index]\n",
        "        for dim, diag in enumerate(diagram_list):\n",
        "             persistence = diag[:, 1] - diag[:, 0]\n",
        "             finite_persistence = persistence[np.isfinite(persistence)]\n",
        "             summary[f\"H{dim}_features\"] = diag.shape[0]\n",
        "             summary[f\"H{dim}_total_persistence\"] = float(np.sum(finite_persistence)) if finite_persistence.size > 0 else 0.0\n",
        "             summary[f\"H{dim}_mean_persistence\"] = float(np.mean(finite_persistence)) if finite_persistence.size > 0 else 0.0\n",
        "        return summary\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        serializable_history = [[d.tolist() for d in diag_list] if diag_list is not None else None for diag_list in self.persistence_diagrams_history]\n",
        "        return { \"version\": FRAMEWORK_VERSION, \"max_dimension\": self.max_dimension, \"persistence_diagrams_history\": serializable_history }\n",
        "\n",
        "    def load_state(self, state: Dict):\n",
        "        loaded_version = state.get(\"version\");\n",
        "        if loaded_version != FRAMEWORK_VERSION: warnings.warn(f\"Loading TDA v '{loaded_version}' into v '{FRAMEWORK_VERSION}'.\", UserWarning)\n",
        "        self.max_dimension = state.get(\"max_dimension\", self.max_dimension); loaded_history = state.get(\"persistence_diagrams_history\", [])\n",
        "        self.persistence_diagrams_history = [[np.array(d) for d in diag_list_data] if diag_list_data is not None else None for diag_list_data in loaded_history]\n",
        "        self.has_tda_lib = HAS_RIPSER # Re-check on load\n",
        "\n"
      ],
      "metadata": {
        "id": "oxArJZ3iyIQC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/analysis/visualization.py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import List, Dict, Optional, Tuple, Any\n",
        "\n",
        "# TDA Dependency Handling\n",
        "try: import persim; HAS_PERSIM = True\n",
        "except ImportError: HAS_PERSIM = False\n",
        "\n",
        "def plot_persistence_diagram(diagram: np.ndarray, title: str = \"Persistence Diagram\", ax: Optional[plt.Axes] = None, show_plot: bool = True, **kwargs):\n",
        "    \"\"\" Plots a persistence diagram using matplotlib. \"\"\"\n",
        "    if diagram is None or diagram.ndim != 2 or diagram.shape[1] != 2 or diagram.shape[0] == 0: warnings.warn(f\"Invalid diagram for '{title}'. Skip plot.\", RuntimeWarning); return\n",
        "    if ax is None: fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    finite_vals = diagram[np.isfinite(diagram)]; min_val = np.min(finite_vals) if finite_vals.size > 0 else 0; max_val = np.max(finite_vals) if finite_vals.size > 0 else 1\n",
        "    plot_diagram = diagram.copy(); inf_death_val = max_val + 0.1 * (max_val - min_val + 1e-6); inf_indices = np.isinf(plot_diagram[:, 1]); plot_diagram[inf_indices, 1] = inf_death_val\n",
        "    ax.scatter(plot_diagram[:, 0], plot_diagram[:, 1], **kwargs)\n",
        "    lim_min = min_val - 0.05 * (max_val - min_val + 1e-6); lim_max = inf_death_val + 0.05 * (max_val - min_val + 1e-6)\n",
        "    ax.plot([lim_min, lim_max], [lim_min, lim_max], '--', color='grey', label='y=x')\n",
        "    ax.set_xlabel(\"Birth\"); ax.set_ylabel(\"Death\"); ax.set_title(title); ax.set_aspect('equal', adjustable='box'); ax.grid(True, linestyle=':', alpha=0.6); ax.legend()\n",
        "    if show_plot: plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_persistence_barcode(diagram: np.ndarray, title: str = \"Persistence Barcode\", ax: Optional[plt.Axes] = None, show_plot: bool = True, **kwargs):\n",
        "    \"\"\" Plots a persistence barcode using matplotlib. \"\"\"\n",
        "    if diagram is None or diagram.ndim != 2 or diagram.shape[1] != 2 or diagram.shape[0] == 0: warnings.warn(f\"Invalid diagram for '{title}'. Skip plot.\", RuntimeWarning); return\n",
        "    if ax is None: fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    sorted_diagram = diagram[np.argsort(diagram[:, 0])]; plot_diagram = sorted_diagram.copy()\n",
        "    finite_deaths = plot_diagram[np.isfinite(plot_diagram[:, 1]), 1]; max_finite_death = np.max(finite_deaths) if finite_deaths.size > 0 else np.max(plot_diagram[:, 0])\n",
        "    inf_death_val = max_finite_death + 0.1 * (max_finite_death - np.min(plot_diagram[:,0]) + 1e-6); inf_indices = np.isinf(plot_diagram[:, 1]); plot_diagram[inf_indices, 1] = inf_death_val\n",
        "    for i, (birth, death) in enumerate(plot_diagram): ax.hlines(y=i, xmin=birth, xmax=death, linewidth=2, **kwargs)\n",
        "    ax.set_xlabel(\"Time (Radius/Scale)\"); ax.set_ylabel(\"Feature Index\"); ax.set_title(title); ax.set_yticks([])\n",
        "    if show_plot: plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_metric_comparison(epoch_history: Dict[int, Dict[str, Any]], metrics_to_plot: List[str],\n",
        "                           tda_metric_history: Optional[Dict[int, Dict[str, Any]]] = None, tda_metrics_to_plot: Optional[List[str]] = None,\n",
        "                           title: str = \"Simulation Metrics Over Time\", figsize: Tuple[int, int] = (12, 6)):\n",
        "    \"\"\" Plots specified simulation metrics and optional TDA metrics over epochs. \"\"\"\n",
        "    if not epoch_history: warnings.warn(\"Empty epoch history. Cannot plot.\", RuntimeWarning); return\n",
        "    epochs = sorted(epoch_history.keys()); num_metrics = len(metrics_to_plot); num_tda_metrics = len(tda_metrics_to_plot) if tda_metrics_to_plot else 0; total_plots = num_metrics + num_tda_metrics\n",
        "    if total_plots == 0: warnings.warn(\"No metrics specified for plotting.\", RuntimeWarning); return\n",
        "    fig, axes = plt.subplots(total_plots, 1, figsize=figsize, sharex=True, squeeze=False); fig.suptitle(title, fontsize=14); plot_idx = 0\n",
        "    # Plot general metrics\n",
        "    for metric_key in metrics_to_plot:\n",
        "        ax = axes[plot_idx, 0]; values = [epoch_history[e].get(metric_key) for e in epochs]; valid_epochs = [e for i, e in enumerate(epochs) if values[i] is not None and np.isfinite(values[i])]; valid_values = [v for v in values if v is not None and np.isfinite(v)]\n",
        "        if valid_values: ax.plot(valid_epochs, valid_values, marker='.', linestyle='-', label=metric_key); ax.legend(loc='upper left')\n",
        "        else: ax.text(0.5, 0.5, f\"No data for '{metric_key}'\", ha='center', va='center', transform=ax.transAxes)\n",
        "        ax.set_ylabel(metric_key.replace('_', ' ').title()); ax.grid(True, linestyle=':', alpha=0.6); plot_idx += 1\n",
        "    # Plot TDA metrics\n",
        "    if tda_metric_history and tda_metrics_to_plot:\n",
        "        tda_epochs = sorted(tda_metric_history.keys())\n",
        "        for metric_key in tda_metrics_to_plot:\n",
        "             ax = axes[plot_idx, 0]; values = [tda_metric_history[e].get(metric_key) for e in tda_epochs if e in tda_metric_history]; valid_epochs = [e for e in tda_epochs if e in tda_metric_history and tda_metric_history[e].get(metric_key) is not None and np.isfinite(tda_metric_history[e].get(metric_key))]; valid_values = [tda_metric_history[e].get(metric_key) for e in valid_epochs]\n",
        "             if valid_values: ax.plot(valid_epochs, valid_values, marker='x', linestyle='--', label=f\"TDA: {metric_key}\", color='red'); ax.legend(loc='upper left')\n",
        "             else: ax.text(0.5, 0.5, f\"No data for TDA '{metric_key}'\", ha='center', va='center', transform=ax.transAxes)\n",
        "             ax.set_ylabel(metric_key.replace('_', ' ').title()); ax.grid(True, linestyle=':', alpha=0.6); plot_idx += 1\n",
        "    axes[-1, 0].set_xlabel(\"Simulation Epoch\"); plt.tight_layout(rect=[0, 0.03, 1, 0.97]); plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "I31GO7LEyLCN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/analysis/clustering.py\n",
        "import numpy as np\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Optional, TYPE_CHECKING\n",
        "\n",
        "# Use TYPE_CHECKING for chronicler hint\n",
        "if TYPE_CHECKING:\n",
        "    from ..chronicler import ResonanceChronicler # Adjust if chronicler is moved\n",
        "\n",
        "# Dependency check\n",
        "try:\n",
        "    from sklearn.cluster import KMeans\n",
        "    HAS_SKLEARN = True\n",
        "except ImportError:\n",
        "    HAS_SKLEARN = False\n",
        "    warnings.warn(\"`scikit-learn` not found. Archetype clustering disabled.\", ImportWarning)\n",
        "\n",
        "def cluster_archetypes(chronicler: 'ResonanceChronicler', n_clusters: int = 5, min_samples: int = 10) -> Optional[Dict[int, List[str]]]:\n",
        "    \"\"\"\n",
        "    Clusters agent style vectors recorded by the chronicler using KMeans\n",
        "    to identify emergent archetypes based on linguistic style.\n",
        "\n",
        "    Args:\n",
        "        chronicler: The ResonanceChronicler instance containing simulation history.\n",
        "        n_clusters: The target number of clusters (archetypes) to find.\n",
        "        min_samples: Minimum number of style vectors required to attempt clustering.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping cluster label (int) to a list of agent IDs belonging\n",
        "        predominantly to that cluster, or None if clustering fails or insufficient data.\n",
        "    \"\"\"\n",
        "    if not HAS_SKLEARN:\n",
        "        warnings.warn(\"Cannot cluster archetypes: scikit-learn not installed.\", RuntimeWarning)\n",
        "        return None\n",
        "\n",
        "    all_vectors = []; agent_epoch_ids = []\n",
        "    # Extract valid style vectors (ensure they are lists/convert back to numpy)\n",
        "    for agent_id, lineage in chronicler.lineages.items():\n",
        "        for i, entry in enumerate(lineage):\n",
        "            if entry.style_vector is not None:\n",
        "                 try:\n",
        "                     # Ensure vector is numpy array for clustering\n",
        "                     vec = np.array(entry.style_vector)\n",
        "                     if vec.ndim == 1: # Check if it's a 1D vector\n",
        "                          all_vectors.append(vec)\n",
        "                          agent_epoch_ids.append((agent_id, i))\n",
        "                 except Exception as e:\n",
        "                      warnings.warn(f\"Skipping invalid style vector for {agent_id} epoch {i}: {e}\", RuntimeWarning)\n",
        "\n",
        "    if len(all_vectors) < max(n_clusters, min_samples):\n",
        "        warnings.warn(f\"Insufficient valid data ({len(all_vectors)}) for clustering into {n_clusters} clusters.\", RuntimeWarning)\n",
        "        return None\n",
        "\n",
        "    print(f\"Clustering {len(all_vectors)} style vectors into {n_clusters} archetypes using KMeans...\")\n",
        "    all_vectors_array = np.array(all_vectors)\n",
        "\n",
        "    try:\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto').fit(all_vectors_array) # Use 'auto' n_init\n",
        "        labels = kmeans.labels_\n",
        "        # Map agents to their most frequent cluster label\n",
        "        agent_cluster_counts = defaultdict(lambda: defaultdict(int))\n",
        "        for i, label in enumerate(labels):\n",
        "            agent_id, _ = agent_epoch_ids[i]\n",
        "            agent_cluster_counts[agent_id][label] += 1\n",
        "\n",
        "        # Assign agent to the cluster they appeared in most often\n",
        "        agent_dominant_cluster = {}\n",
        "        for agent_id, counts in agent_cluster_counts.items():\n",
        "            dominant_label = max(counts, key=counts.get)\n",
        "            agent_dominant_cluster[agent_id] = dominant_label\n",
        "\n",
        "        # Group agents by their dominant cluster\n",
        "        clustered_agents = defaultdict(list)\n",
        "        for agent_id, label in agent_dominant_cluster.items():\n",
        "            clustered_agents[label].append(agent_id)\n",
        "\n",
        "        print(f\"Clustering successful. Found {len(clustered_agents)} clusters.\")\n",
        "        # Return dict mapping cluster label to list of agent IDs primarily in that cluster\n",
        "        return dict(clustered_agents)\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"KMeans clustering failed: {e}\", RuntimeWarning)\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "7dcYikOjyNqo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/agisa_sac/orchestrator.py\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Optional, Any, Callable\n",
        "\n",
        "# Import framework version and components using relative paths\n",
        "try:\n",
        "    from . import FRAMEWORK_VERSION\n",
        "    from .agent import EnhancedAgent\n",
        "    from .utils.message_bus import MessageBus\n",
        "    from .components.social import DynamicSocialGraph\n",
        "    from .chronicler import ResonanceChronicler # Assuming chronicler moved to top level\n",
        "    from .analysis.analyzer import AgentStateAnalyzer\n",
        "    from .analysis.tda import PersistentHomologyTracker\n",
        "    # Check optional dependencies status (assuming defined in __init__ or config)\n",
        "    # from . import HAS_CUPY, HAS_SENTENCE_TRANSFORMER\n",
        "    HAS_CUPY = False # Placeholder\n",
        "    HAS_SENTENCE_TRANSFORMER = False # Placeholder\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Could not import necessary AGI-SAC components for Orchestrator: {e}\")\n",
        "\n",
        "\n",
        "class SimulationOrchestrator:\n",
        "    \"\"\" Manages the setup, execution, state, and analysis of the AGI-SAC simulation. \"\"\"\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.num_agents = config.get('num_agents', 100)\n",
        "        self.num_epochs = config.get('num_epochs', 50)\n",
        "        seed = config.get('random_seed')\n",
        "        if seed is not None: random.seed(seed); np.random.seed(seed)\n",
        "\n",
        "        self.message_bus = MessageBus()\n",
        "        self.agent_ids = [f\"agent_{i}\" for i in range(self.num_agents)]\n",
        "        # Agent creation requires config details\n",
        "        self.agents: Dict[str, EnhancedAgent] = self._create_agents()\n",
        "        self.social_graph = DynamicSocialGraph(self.num_agents, self.agent_ids, use_gpu=config.get('use_gpu', False) and HAS_CUPY, message_bus=self.message_bus)\n",
        "        self.chronicler = ResonanceChronicler()\n",
        "        self.analyzer = AgentStateAnalyzer(self.agents) # Pass agents dict\n",
        "        self.tda_tracker = PersistentHomologyTracker(max_dimension=config.get('tda_max_dimension', 1))\n",
        "\n",
        "        self.current_epoch = 0; self.is_running = False; self.simulation_start_time = None\n",
        "        self.hooks: Dict[str, List[Callable]] = defaultdict(list)\n",
        "        print(f\"SimulationOrchestrator initialized ({FRAMEWORK_VERSION}) with {self.num_agents} agents. TDA: {self.tda_tracker.has_tda_lib}, GPU: {self.social_graph.use_gpu}\")\n",
        "\n",
        "\n",
        "    def _create_agents(self) -> Dict[str, EnhancedAgent]:\n",
        "        agents = {}; personalities = self.config.get('personalities', [])\n",
        "        if len(personalities) != self.num_agents:\n",
        "             warnings.warn(f\"Personality count mismatch. Generating random.\", RuntimeWarning); personalities = [{\"openness\": random.uniform(0.3, 0.7), \"consistency\": random.uniform(0.4, 0.6),\"conformity\": random.uniform(0.2, 0.8), \"curiosity\": random.uniform(0.4, 0.9)} for _ in range(self.num_agents)]\n",
        "        agent_capacity = self.config.get('agent_capacity', 100); use_semantic = self.config.get('use_semantic', True) and HAS_SENTENCE_TRANSFORMER\n",
        "        for i, agent_id in enumerate(self.agent_ids):\n",
        "             agents[agent_id] = EnhancedAgent(agent_id=agent_id, personality=personalities[i], capacity=agent_capacity, message_bus=self.message_bus, use_semantic=use_semantic, add_initial_memory=True)\n",
        "        return agents\n",
        "\n",
        "    # ... (register_hook, _trigger_hooks, run_epoch, run_simulation, inject_protocol, get_summary_metrics methods as defined in agisa_orchestrator_serialization_v1) ...\n",
        "    def register_hook(self, hook_point: str, callback: Callable):\n",
        "        valid_hooks = {'pre_epoch', 'post_epoch', 'pre_agent_step', 'post_agent_step', 'simulation_end', 'pre_protocol_injection', 'post_protocol_injection', 'tda_phase_transition'};\n",
        "        if hook_point not in valid_hooks: warnings.warn(f\"Invalid hook point: {hook_point}\", RuntimeWarning); return; self.hooks[hook_point].append(callback)\n",
        "    def _trigger_hooks(self, hook_point: str, **kwargs):\n",
        "        if hook_point in self.hooks:\n",
        "            for callback in self.hooks[hook_point]:\n",
        "                try: callback(orchestrator=self, epoch=self.current_epoch, **kwargs); except Exception as e: warnings.warn(f\"Hook error '{hook_point}' ({callback.__name__}): {e}\", RuntimeWarning)\n",
        "    def run_epoch(self):\n",
        "        if not self.is_running: warnings.warn(\"Sim not running.\", RuntimeWarning); return; epoch_start_time = time.time(); self._trigger_hooks('pre_epoch'); situational_entropy = random.uniform(0.1, 0.7)\n",
        "        agent_order = list(self.agents.keys()); random.shuffle(agent_order); cognitive_states_for_tda = []\n",
        "        for agent_id in agent_order:\n",
        "            agent = self.agents.get(agent_id);\n",
        "            if not agent: continue\n",
        "            self._trigger_hooks('pre_agent_step', agent_id=agent_id); peer_influence = self.social_graph.get_peer_influence_for_agent(agent_id, normalize=True); query = f\"Epoch {self.current_epoch+1} status. E:{situational_entropy:.2f}\"\n",
        "            agent.simulation_step(situational_entropy, peer_influence, query); self.chronicler.record_epoch(agent, self.current_epoch);\n",
        "            if hasattr(agent, 'cognitive') and agent.cognitive.cognitive_state is not None: cognitive_states_for_tda.append(agent.cognitive.cognitive_state)\n",
        "            self._trigger_hooks('post_agent_step', agent_id=agent_id)\n",
        "        # TDA\n",
        "        tda_run_freq = self.config.get('tda_run_frequency', 1)\n",
        "        if self.tda_tracker and (self.current_epoch + 1) % tda_run_freq == 0:\n",
        "            if len(cognitive_states_for_tda) > 1:\n",
        "                 point_cloud = np.array(cognitive_states_for_tda); max_radius = self.config.get('tda_max_radius', None)\n",
        "                 diagrams = self.tda_tracker.compute_persistence(point_cloud, max_radius=max_radius)\n",
        "                 if diagrams is not None:\n",
        "                      distance_metric = self.config.get('tda_distance_metric', 'bottleneck'); comparison_dim = self.config.get('tda_comparison_dimension', 1); threshold = self.config.get('tda_transition_threshold', 0.2)\n",
        "                      transition_detected, distance = self.tda_tracker.detect_phase_transition(comparison_dimension=comparison_dim, distance_metric=distance_metric, threshold=threshold)\n",
        "                      if transition_detected: print(f\"!!! Epoch {self.current_epoch+1}: TDA Phase transition (Dim={comparison_dim}, Dist={distance:.3f} > {threshold}) !!!\"); self._trigger_hooks('tda_phase_transition', dimension=comparison_dim, metric=distance_metric, distance=distance)\n",
        "            else: self.tda_tracker.persistence_diagrams_history.append(None) # Keep history aligned\n",
        "        # Communities\n",
        "        community_check_freq = self.config.get('community_check_frequency', 5)\n",
        "        if (self.current_epoch + 1) % community_check_freq == 0: self.social_graph.detect_communities()\n",
        "        self._trigger_hooks('post_epoch'); epoch_duration = time.time() - epoch_start_time\n",
        "        log_freq = self.config.get('epoch_log_frequency', 10)\n",
        "        if (self.current_epoch + 1) % log_freq == 0 or self.current_epoch == 0: print(f\"--- Epoch {self.current_epoch+1}/{self.num_epochs} completed [{epoch_duration:.2f}s] ---\")\n",
        "    def run_simulation(self, num_epochs: Optional[int] = None): # Allow overriding num_epochs\n",
        "        run_epochs = num_epochs if num_epochs is not None else self.num_epochs\n",
        "        if run_epochs <= 0: print(\"No epochs to run.\"); return\n",
        "        print(f\"\\n--- Starting Simulation Run ({run_epochs} Epochs) ---\"); self.is_running = True; self.simulation_start_time = time.time(); start_epoch = self.current_epoch\n",
        "        for epoch in range(start_epoch, start_epoch + run_epochs):\n",
        "             if epoch >= self.num_epochs: print(f\"Reached configured max epochs ({self.num_epochs}). Stopping.\"); break\n",
        "             self.current_epoch = epoch; self.run_epoch()\n",
        "        self.is_running = False; total_duration = time.time() - self.simulation_start_time; print(f\"\\n--- Simulation Run Complete ({total_duration:.2f} seconds) ---\"); self._trigger_hooks('simulation_end')\n",
        "    def inject_protocol(self, protocol_name: str, parameters: Dict):\n",
        "        # ... (logic as defined in agisa_orchestrator_protocol_v1) ...\n",
        "        print(f\"Injecting protocol '{protocol_name}'\"); self._trigger_hooks('pre_protocol_injection', protocol_name=protocol_name, parameters=parameters);\n",
        "        if protocol_name == \"divergence_stress\": target_agents = self._select_agents_for_protocol(parameters); if not target_agents: print(\"No agents selected.\"); return; heuristic_mult_range = parameters.get(\"heuristic_multiplier_range\", (0.5, 0.8)); counter_narrative = parameters.get(\"counter_narrative\", \"Ghosts...\"); narrative_importance = parameters.get(\"narrative_importance\", 0.9); narrative_theme = parameters.get(\"narrative_theme\", \"divergence_seed\"); print(f\"Applying stress to {len(target_agents)} agents...\"); modified_count = 0; for agent in target_agents: try: multiplier = random.uniform(heuristic_mult_range[0], heuristic_mult_range[1]); agent.cognitive.heuristics *= multiplier; agent.cognitive.heuristics = 1 / (1 + np.exp(-agent.cognitive.heuristics)); agent.cognitive.heuristics = np.clip(agent.cognitive.heuristics, 0.1, 0.9); agent.memory.add_memory(content={\"type\": \"divergence_seed\", \"source\": \"SYS_PROTO\", \"text\": counter_narrative, \"theme\": narrative_theme, \"timestamp\": time.time()}, importance=narrative_importance); modified_count += 1; except Exception as e: warnings.warn(f\"Stress failed for {agent.agent_id}: {e}\", RuntimeWarning); print(f\"Stress applied to {modified_count} agents.\");\n",
        "        elif protocol_name == \"satori_probe\": threshold = parameters.get('threshold', self.config.get('satori_threshold_analyzer', 0.88)); ratio = self.analyzer.compute_satori_wave_ratio(threshold=threshold) if self.analyzer else 0.0; print(f\"Satori Probe (Thresh {threshold}): {ratio:.3f}\");\n",
        "        elif protocol_name == \"echo_fusion\": print(\"Echo Fusion TBD.\"); pass;\n",
        "        elif protocol_name == \"satori_lattice\": print(\"Satori Lattice TBD.\"); pass;\n",
        "        else: warnings.warn(f\"Unknown protocol: {protocol_name}\", RuntimeWarning);\n",
        "        self._trigger_hooks('post_protocol_injection', protocol_name=protocol_name, parameters=parameters)\n",
        "    def get_summary_metrics(self, satori_threshold: Optional[float] = None) -> Dict[str, Any]:\n",
        "        if not self.analyzer: return {\"error\": \"Analyzer N/A.\"}; threshold = satori_threshold if satori_threshold is not None else self.config.get('satori_threshold_analyzer', 0.88); return self.analyzer.summarize(satori_threshold=threshold)\n",
        "    def save_state(self, filepath: str, include_memory_embeddings: bool = False, resonance_history_limit: Optional[int] = 100) -> bool:\n",
        "        # ... (logic from agisa_orchestrator_serialization_v1) ...\n",
        "        if self.is_running: warnings.warn(\"Saving state while running.\", RuntimeWarning); try: print(f\"Saving state to {filepath}...\"); state = { \"framework_version\": FRAMEWORK_VERSION, \"config\": self.config, \"current_epoch\": self.current_epoch, \"agents_state\": {aid: agent.to_dict(include_memory_embeddings=include_memory_embeddings, resonance_history_limit=resonance_history_limit) for aid, agent in self.agents.items()}, \"social_graph_state\": self.social_graph.to_dict(), \"chronicler_state\": self.chronicler.to_dict(), \"tda_tracker_state\": self.tda_tracker.to_dict() if self.tda_tracker else None, \"random_state\": random.getstate(), \"numpy_random_state\": np.random.get_state(), \"cupy_random_state\": cp.random.get_random_state().get_state() if HAS_CUPY and self.config.get('use_gpu', False) else None, }; with open(filepath, 'wb') as f: pickle.dump(state, f); print(f\"State saved.\"); return True; except Exception as e: warnings.warn(f\"Save failed: {e}\", category=RuntimeWarning); import traceback; traceback.print_exc(); return False\n",
        "    def load_state(self, filepath: str) -> bool:\n",
        "        # ... (logic from agisa_orchestrator_serialization_v1) ...\n",
        "        if self.is_running: warnings.warn(\"Loading state while running.\", RuntimeWarning); try: print(f\"Loading state from {filepath}...\"); with open(filepath, 'rb') as f: state = pickle.load(f); loaded_framework_version = state.get(\"framework_version\"); if loaded_framework_version != FRAMEWORK_VERSION: warnings.warn(f\"Version mismatch: loading '{loaded_framework_version}' into '{FRAMEWORK_VERSION}'.\", category=UserWarning); self.config = state.get(\"config\", self.config); self.num_agents = self.config.get('num_agents', 100); self.num_epochs = self.config.get('num_epochs', 50); self.agent_ids = [f\"agent_{i}\" for i in range(self.num_agents)]; self.current_epoch = state.get(\"current_epoch\", 0); if \"random_state\" in state: random.setstate(state[\"random_state\"]); if \"numpy_random_state\" in state: np.random.set_state(state[\"numpy_random_state\"]); if HAS_CUPY and \"cupy_random_state\" in state and state[\"cupy_random_state\"] is not None: cupy_state = cp.random.get_random_state(); cupy_state.set_state(state[\"cupy_random_state\"]); agents_state_data = state.get(\"agents_state\", {}); self.agents = {}; print(f\"Reconstructing {len(agents_state_data)} agents...\"); for agent_id, agent_data in agents_state_data.items(): if agent_id not in self.agent_ids: warnings.warn(f\"Skipping agent {agent_id} from save (not in current config).\"); continue; try: self.agents[agent_id] = EnhancedAgent.from_dict(agent_data, self.message_bus, strict_validation=False); except Exception as e: warnings.warn(f\"Failed to load agent {agent_id}: {e}\", RuntimeWarning); self.social_graph = DynamicSocialGraph(self.num_agents, self.agent_ids, use_gpu=self.config.get('use_gpu', False), message_bus=self.message_bus); if \"social_graph_state\" in state: self.social_graph.load_state(state[\"social_graph_state\"]); self.chronicler = ResonanceChronicler(); if \"chronicler_state\" in state: self.chronicler.load_state(state[\"chronicler_state\"]); self.tda_tracker = PersistentHomologyTracker(max_dimension=self.config.get('tda_max_dimension', 1)); if \"tda_tracker_state\" in state and self.tda_tracker: self.tda_tracker.load_state(state[\"tda_tracker_state\"]); self.analyzer = AgentStateAnalyzer(self.agents); self.is_running = False; print(f\"State loaded. Resuming at epoch {self.current_epoch + 1}.\"); return True; except FileNotFoundError: warnings.warn(f\"Load failed: File not found at {filepath}\", category=RuntimeWarning); return False; except Exception as e: warnings.warn(f\"Load failed: {e}\", category=RuntimeWarning); import traceback; traceback.print_exc(); return False\n",
        "    def _select_agents_for_protocol(self, parameters: Dict) -> List[EnhancedAgent]:\n",
        "        # ... (logic from agisa_orchestrator_protocol_v1) ...\n",
        "        selection_method = parameters.get(\"selection_method\", \"percentage\"); target_agents = []; agent_list = list(self.agents.values())\n",
        "        if not agent_list: return []\n",
        "        if selection_method == \"percentage\":\n",
        "            percentage = parameters.get(\"percentage\", 0.1); count = max(1, int(self.num_agents * percentage)); basis = parameters.get(\"selection_basis\", \"random\")\n",
        "            if basis == \"random\": target_agents = random.sample(agent_list, min(count, self.num_agents))\n",
        "            else: warnings.warn(f\"Unsupported basis '{basis}'. Default random.\", RuntimeWarning); target_agents = random.sample(agent_list, min(count, self.num_agents))\n",
        "        else: warnings.warn(f\"Unknown selection method '{selection_method}'.\", RuntimeWarning)\n",
        "        print(f\"Selected {len(target_agents)} agents for protocol via '{selection_method}'.\")\n",
        "        return target_agents\n",
        "\n"
      ],
      "metadata": {
        "id": "iDke5k1_yRMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "4eca15d6-334e-4263-f1bd-096e3c7e8779"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-38-405776702.py, line 67)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-38-405776702.py\"\u001b[0;36m, line \u001b[0;32m67\u001b[0m\n\u001b[0;31m    try: callback(orchestrator=self, epoch=self.current_epoch, **kwargs); except Exception as e: warnings.warn(f\"Hook error '{hook_point}' ({callback.__name__}): {e}\", RuntimeWarning)\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "674cb59f",
        "outputId": "3792cb98-9ea9-41b8-8463-1c2fc281e453"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the 'src' directory to the Python path\n",
        "# This assumes the notebook is run from the root of the project structure,\n",
        "# or that 'src' is a subdirectory of the current working directory.\n",
        "if 'src' not in sys.path:\n",
        "    sys.path.insert(0, os.path.abspath('src'))\n",
        "    print(\"Added 'src' to sys.path\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'src' to sys.path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62aedeea"
      },
      "source": [
        "# Remove any previous clones to avoid nested directories or old versions\n",
        "!rm -rf /content/agisa_sac\n",
        "!rm -rf /content/sample_data # Clean up default colab directory if needed"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb333589",
        "outputId": "274d7584-98e8-4d49-c76d-7757c4628403"
      },
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/google-research/agisa_sac.git /content/agisa_sac"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/agisa_sac'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69590279",
        "outputId": "0a39ac98-11f0-485a-8904-f6f1fbaf7225"
      },
      "source": [
        "# Change directory to the repository root\n",
        "%cd /content/agisa_sac"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/agisa_sac'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ea5522"
      },
      "source": [
        "### 2. Install Dependencies\n",
        "\n",
        "This cell installs the required Python packages, including the project itself in editable mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7e60436"
      },
      "source": [
        "get_ipython().system('git config --global user.name topstolenname')\n",
        "get_ipython().system('git config --global user.email jessupjdj@outlook.com')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5e626d"
      },
      "source": [
        "# Remove any previous clones to avoid nested directories or old versions\n",
        "!rm -rf /content/agisa_sac\n",
        "!rm -rf /content/sample_data # Clean up default colab directory if needed"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "437cf3e9",
        "outputId": "6ee795e2-30a7-4162-daca-e95c4f246fd6"
      },
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/google-research/agisa_sac.git /content/agisa_sac"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/agisa_sac'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b2b49bf",
        "outputId": "94f0b14c-b207-482e-a03e-0515e2b8160a"
      },
      "source": [
        "# Change directory to the repository root\n",
        "%cd /content/agisa_sac"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/agisa_sac'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd863e95"
      },
      "source": [
        "### 2. Install Dependencies\n",
        "\n",
        "This cell installs the required Python packages, including the project itself in editable mode."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gh repo clone topstolenname/agisa_sac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzlkYAvQcP6_",
        "outputId": "c514467b-bd5e-40d0-add6-983ff63dd466"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;1;39mWelcome to GitHub CLI!\u001b[0m\n",
            "\n",
            "To authenticate, please run `gh auth login`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t ed25519 -C \"tristan@mindlink.dev\" -f /root/.ssh/id_ed25519 -N \"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac08YjAfdAsc",
        "outputId": "10d35019-b7ed-46ad-e1ff-f1d59c9ded47"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private ed25519 key pair.\n",
            "/root/.ssh/id_ed25519 already exists.\n",
            "Overwrite (y/n)? n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.ssh/id_ed25519.pub\n"
      ],
      "metadata": {
        "id": "6vKrqOzzdqEX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}